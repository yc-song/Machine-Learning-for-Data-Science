{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCYO6dmGgefe"
   },
   "source": [
    "# Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21033,
     "status": "ok",
     "timestamp": 1652416430254,
     "user": {
      "displayName": "­송종현 / 학생 / 데이터사이언스학과",
      "userId": "03940744101866465560"
     },
     "user_tz": -540
    },
    "id": "er0RD438gRLm",
    "outputId": "b1b274c9-02eb-4d94-a21f-a8eea0e68ae0"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "executionInfo": {
     "elapsed": 323,
     "status": "error",
     "timestamp": 1652416912847,
     "user": {
      "displayName": "­송종현 / 학생 / 데이터사이언스학과",
      "userId": "03940744101866465560"
     },
     "user_tz": -540
    },
    "id": "Jfeql_8sgnKJ",
    "outputId": "04c3be9a-477e-4ee0-a70f-ac3e18b7075a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nChange directory to where this file is located\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Change directory to where this file is located\n",
    "\"\"\"\n",
    "#%cd 'COPY&PASTE FILE DIRECTORY HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11725,
     "status": "ok",
     "timestamp": 1652417249745,
     "user": {
      "displayName": "­송종현 / 학생 / 데이터사이언스학과",
      "userId": "03940744101866465560"
     },
     "user_tz": -540
    },
    "id": "DMnPiwtCcF1j",
    "outputId": "e6f71c1a-9ca4-4004-d141-a733216e31a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open mnist.zipy, mnist.zipy.zip or mnist.zipy.ZIP.\r\n"
     ]
    }
   ],
   "source": [
    "!unzip mnist.zipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPEoabX-hGCh"
   },
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OyammZP8hI7P"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist.data_utils import load_data\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19i02l2-cFIg"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLxTNOvI5NHD"
   },
   "source": [
    "#Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xuQB6W2U5ZE2"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Do NOT modify this function\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def softmax(X):\n",
    "    \"\"\"\n",
    "    Do NOT modify this function\n",
    "    \"\"\"\n",
    "    logit = np.exp(X-np.amax(X, axis=1, keepdims=True))\n",
    "    numer = logit\n",
    "    denom = np.sum(logit, axis=1, keepdims=True)\n",
    "    return numer/denom\n",
    "\n",
    "def load_batch(X, Y, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates batches with the remainder dropped.\n",
    "\n",
    "    Do NOT modify this function\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        permutation = np.random.permutation(X.shape[0])\n",
    "        X = X[permutation, :]\n",
    "        Y = Y[permutation, :]\n",
    "    num_steps = int(X.shape[0])//batch_size\n",
    "    step = 0\n",
    "    while step<num_steps:\n",
    "        X_batch = X[batch_size*step:batch_size*(step+1)]\n",
    "        Y_batch = Y[batch_size*step:batch_size*(step+1)]\n",
    "        step+=1\n",
    "        yield X_batch, Y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsU8v_6khR30"
   },
   "source": [
    "\n",
    "\n",
    "#2-Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mA5udiGmhRb5"
   },
   "outputs": [],
   "source": [
    "class TwoLayerNN:\n",
    "    \"\"\" a neural network with 2 layers \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_hiddens, num_classes):\n",
    "        \"\"\"\n",
    "        Do NOT modify this function.\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_classes = num_classes\n",
    "        self.params = self.initialize_parameters(input_dim, num_hiddens, num_classes)\n",
    "    def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n",
    "        \"\"\"\n",
    "        initializes parameters with Xavier Initialization.\n",
    "\n",
    "        Question (a)\n",
    "        - refer to https://paperswithcode.com/method/xavier-initialization for Xavier initialization \n",
    "        \n",
    "        Inputs\n",
    "        - input_dim\n",
    "        - num_hiddens\n",
    "        - num_classes\n",
    "        Returns\n",
    "        - params: a dictionary with the initialized parameters.\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "        params[\"W1\"]=np.random.uniform(-1/np.sqrt(input_dim), 1/np.sqrt(input_dim),(input_dim,num_hiddens))\n",
    "        params[\"W2\"]=np.random.uniform(-1/np.sqrt(num_hiddens), 1/np.sqrt(num_hiddens),(num_hiddens,num_classes))\n",
    "        params[\"b1\"]=np.zeros(num_hiddens)\n",
    "        params[\"b2\"]=np.zeros(num_classes)\n",
    "\n",
    "\n",
    "        return params\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Define and perform the feed forward step of a two-layer neural network.\n",
    "        Specifically, the network structue is given by\n",
    "\n",
    "          y = softmax(sigmoid(X W1 + b1) W2 + b2)\n",
    "\n",
    "        where X is the input matrix of shape (N, D), y is the class distribution matrix\n",
    "        of shape (N, C), N is the number of examples (either the entire dataset or\n",
    "        a mini-batch), D is the feature dimensionality, and C is the number of classes.\n",
    "\n",
    "        Question (b)\n",
    "        - ff_dict will be used to run backpropagation in backward method.\n",
    "\n",
    "        Inputs\n",
    "        - X: the input matrix of shape (N, D)\n",
    "\n",
    "        Returns\n",
    "        - y: the output of the model\n",
    "        - ff_dict: a dictionary with all the fully connected units and activations.\n",
    "        \"\"\"\n",
    "        ff_dict = {}\n",
    "        ff_dict=self.params\n",
    "        ff_dict[\"h\"]=sigmoid((np.matmul(X,ff_dict[\"W1\"])+ff_dict[\"b1\"]))\n",
    "\n",
    "        ff_dict[\"minus h\"]=1-sigmoid((np.matmul(X,ff_dict[\"W1\"])+ff_dict[\"b1\"]))\n",
    "\n",
    "        y=softmax(np.matmul(ff_dict[\"h\"],ff_dict[\"W2\"])+ff_dict[\"b2\"])\n",
    "        ff_dict[\"y\"]=y\n",
    "        \n",
    "        return y, ff_dict\n",
    "\n",
    "    def backward(self, X, Y, ff_dict):\n",
    "        \"\"\"\n",
    "        Performs backpropagation over the two-layer neural network, and returns\n",
    "        a dictionary of gradients of all model parameters.\n",
    "\n",
    "        Question (c)\n",
    "\n",
    "        Inputs:\n",
    "         - X: the input matrix of shape (B, D), where B is the number of examples\n",
    "              in a mini-batch, D is the feature dimensionality.\n",
    "         - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n",
    "              where B is the number of examples in a mini-batch, C is the number\n",
    "              of classes.\n",
    "         - ff_dict: the dictionary containing all the fully connected units and\n",
    "              activations.\n",
    "\n",
    "        Returns:\n",
    "         - grads: a dictionary containing the gradients of corresponding weights and biases.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        grads = {}\n",
    "        grads[\"h\"]=np.matmul(ff_dict[\"y\"]-Y,np.transpose(ff_dict[\"W2\"]))\n",
    "        grads[\"dW2\"]=np.matmul((np.transpose(ff_dict[\"h\"])),ff_dict[\"y\"]-Y)\n",
    "        grads[\"db1\"] =grads[\"h\"]*ff_dict[\"h\"]*ff_dict[\"minus h\"]\n",
    "        grads[\"db1\"]=grads[\"db1\"].sum(0)\n",
    "        grads[\"dW1\"] = np.matmul(np.transpose(X),grads[\"h\"]*ff_dict[\"h\"]*ff_dict[\"minus h\"])\n",
    "        grads[\"db2\"] = (ff_dict[\"y\"]-Y)\n",
    "        grads[\"db2\"] = grads[\"db2\"].sum(0)\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def compute_loss(self, Y, Y_hat):\n",
    "        \"\"\"\n",
    "        Computes cross entropy loss.\n",
    "\n",
    "        Do NOT modify this function.\n",
    "\n",
    "        Inputs\n",
    "            Y:\n",
    "            Y_hat:\n",
    "        Returns\n",
    "            loss:\n",
    "        \"\"\"\n",
    "        loss = -(1/Y.shape[0]) * np.sum(np.multiply(Y, np.log(Y_hat)))\n",
    "        return loss\n",
    "\n",
    "    def train(self, X, Y, X_val, Y_val, lr, n_epochs, batch_size, log_interval=1):\n",
    "        \"\"\"\n",
    "        Runs mini-batch gradient descent.\n",
    "\n",
    "        Do NOT Modify this method.\n",
    "\n",
    "        Inputs\n",
    "        - X\n",
    "        - Y\n",
    "        - X_val\n",
    "        - Y_Val\n",
    "        - lr\n",
    "        - n_epochs\n",
    "        - batch_size\n",
    "        - log_interval\n",
    "        \"\"\"\n",
    "        for epoch in range(n_epochs):\n",
    "            for X_batch, Y_batch in load_batch(X, Y, batch_size):\n",
    "                self.train_step(X_batch, Y_batch, batch_size, lr)\n",
    "            if epoch % log_interval==0:\n",
    "              \n",
    "                Y_hat, ff_dict = self.forward(X)\n",
    "                train_loss = self.compute_loss(Y, Y_hat)\n",
    "                train_acc = self.evaluate(Y, Y_hat)\n",
    "                Y_hat, ff_dict = self.forward(X_val)\n",
    "                valid_loss = self.compute_loss(Y_val, Y_hat)\n",
    "                valid_acc = self.evaluate(Y_val, Y_hat)\n",
    "                print('epoch {:02} - train loss/acc: {:.3f} {:.3f}, valid loss/acc: {:.3f} {:.3f}'.\\\n",
    "                      format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
    "\n",
    "    def train_step(self, X_batch, Y_batch, batch_size, lr):\n",
    "        \"\"\"\n",
    "        Updates the parameters using gradient descent.\n",
    "\n",
    "        Do NOT Modify this method.\n",
    "\n",
    "        Inputs\n",
    "        - X_batch\n",
    "        - Y_batch\n",
    "        - batch_size\n",
    "        - lr\n",
    "        \"\"\"\n",
    "        _, ff_dict = self.forward(X_batch)\n",
    "        grads = self.backward(X_batch, Y_batch, ff_dict)\n",
    "        self.params[\"W1\"] -= lr * grads[\"dW1\"]/batch_size\n",
    "        self.params[\"b1\"] -= lr * grads[\"db1\"]/batch_size\n",
    "        self.params[\"W2\"] -= lr * grads[\"dW2\"]/batch_size\n",
    "        self.params[\"b2\"] -= lr * grads[\"db2\"]/batch_size\n",
    "\n",
    "    def evaluate(self, Y, Y_hat):\n",
    "        \"\"\"\n",
    "        Computes classification accuracy.\n",
    "        \n",
    "        Do NOT modify this function\n",
    "\n",
    "        Inputs\n",
    "        - Y: A numpy array of shape (N, C) containing the softmax outputs,\n",
    "             where C is the number of classes.\n",
    "        - Y_hat: A numpy array of shape (N, C) containing the one-hot encoded labels,\n",
    "             where C is the number of classes.\n",
    "\n",
    "        Returns\n",
    "            accuracy: the classification accuracy in float\n",
    "        \"\"\"        \n",
    "        classes_pred = np.argmax(Y_hat, axis=1)\n",
    "        classes_gt = np.argmax(Y, axis=1)\n",
    "        accuracy = float(np.sum(classes_pred==classes_gt)) / Y.shape[0]\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXM2lWhtDYC6"
   },
   "source": [
    "#Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1520,
     "status": "ok",
     "timestamp": 1652423742614,
     "user": {
      "displayName": "­송종현 / 학생 / 데이터사이언스학과",
      "userId": "03940744101866465560"
     },
     "user_tz": -540
    },
    "id": "48ooR6YIxYhC",
    "outputId": "278703a0-e43b-4c64-db2c-9194d7c5a898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST data loaded:\n",
      "Training data shape: (60000, 784)\n",
      "Training labels shape: (60000, 10)\n",
      "Test data shape: (10000, 784)\n",
      "Test labels shape: (10000, 10)\n",
      "\n",
      "Set validation data aside\n",
      "Training data shape:  (48000, 784)\n",
      "Training labels shape:  (48000, 10)\n",
      "Validation data shape:  (12000, 784)\n",
      "Validation labels shape:  (12000, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, Y_train, X_test, Y_test = load_data()\n",
    "\n",
    "idxs = np.arange(len(X_train))\n",
    "np.random.shuffle(idxs)\n",
    "split_idx = int(np.ceil(len(idxs)*0.8))\n",
    "X_valid, Y_valid = X_train[idxs[split_idx:]], Y_train[idxs[split_idx:]]\n",
    "X_train, Y_train = X_train[idxs[:split_idx]], Y_train[idxs[:split_idx]]\n",
    "print()\n",
    "print('Set validation data aside')\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', Y_train.shape)\n",
    "print('Validation data shape: ', X_valid.shape)\n",
    "print('Validation labels shape: ', Y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzw-D4Zr5xoi"
   },
   "source": [
    "#Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IlnC_rerHPaN"
   },
   "outputs": [],
   "source": [
    "### \n",
    "# Question (d)\n",
    "# Tune the hyperparameters with validation data, \n",
    "# and print the results by running the lines below.\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1652428185715,
     "user": {
      "displayName": "­송종현 / 학생 / 데이터사이언스학과",
      "userId": "03940744101866465560"
     },
     "user_tz": -540
    },
    "id": "TTCqVT4S0Tm5"
   },
   "outputs": [],
   "source": [
    "# model instantiation\n",
    "model = TwoLayerNN(input_dim=784, num_hiddens=256, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 328825,
     "status": "ok",
     "timestamp": 1652428514844,
     "user": {
      "displayName": "­송종현 / 학생 / 데이터사이언스학과",
      "userId": "03940744101866465560"
     },
     "user_tz": -540
    },
    "id": "6cWb6xg0NxOs",
    "outputId": "020c79a3-b013-4f41-e3b1-2eaef465a288"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 00 - train loss/acc: 1.242 0.762, valid loss/acc: 1.242 0.757\n",
      "epoch 01 - train loss/acc: 0.688 0.835, valid loss/acc: 0.693 0.829\n",
      "epoch 02 - train loss/acc: 0.526 0.870, valid loss/acc: 0.534 0.862\n",
      "epoch 03 - train loss/acc: 0.453 0.881, valid loss/acc: 0.462 0.875\n",
      "epoch 04 - train loss/acc: 0.411 0.891, valid loss/acc: 0.419 0.884\n",
      "epoch 05 - train loss/acc: 0.384 0.895, valid loss/acc: 0.394 0.888\n",
      "epoch 06 - train loss/acc: 0.365 0.898, valid loss/acc: 0.375 0.893\n",
      "epoch 07 - train loss/acc: 0.352 0.901, valid loss/acc: 0.361 0.896\n",
      "epoch 08 - train loss/acc: 0.339 0.903, valid loss/acc: 0.349 0.898\n",
      "epoch 09 - train loss/acc: 0.331 0.906, valid loss/acc: 0.342 0.900\n",
      "epoch 10 - train loss/acc: 0.324 0.908, valid loss/acc: 0.334 0.902\n",
      "epoch 11 - train loss/acc: 0.317 0.910, valid loss/acc: 0.328 0.904\n",
      "epoch 12 - train loss/acc: 0.312 0.911, valid loss/acc: 0.323 0.906\n",
      "epoch 13 - train loss/acc: 0.306 0.912, valid loss/acc: 0.317 0.906\n",
      "epoch 14 - train loss/acc: 0.302 0.914, valid loss/acc: 0.313 0.907\n",
      "epoch 15 - train loss/acc: 0.298 0.915, valid loss/acc: 0.309 0.909\n",
      "epoch 16 - train loss/acc: 0.294 0.916, valid loss/acc: 0.305 0.910\n",
      "epoch 17 - train loss/acc: 0.289 0.917, valid loss/acc: 0.302 0.911\n",
      "epoch 18 - train loss/acc: 0.286 0.917, valid loss/acc: 0.299 0.912\n",
      "epoch 19 - train loss/acc: 0.283 0.918, valid loss/acc: 0.297 0.913\n",
      "epoch 20 - train loss/acc: 0.280 0.920, valid loss/acc: 0.293 0.914\n",
      "epoch 21 - train loss/acc: 0.277 0.920, valid loss/acc: 0.290 0.913\n",
      "epoch 22 - train loss/acc: 0.274 0.921, valid loss/acc: 0.289 0.915\n",
      "epoch 23 - train loss/acc: 0.271 0.922, valid loss/acc: 0.286 0.915\n",
      "epoch 24 - train loss/acc: 0.269 0.922, valid loss/acc: 0.283 0.916\n",
      "epoch 25 - train loss/acc: 0.266 0.923, valid loss/acc: 0.281 0.916\n",
      "epoch 26 - train loss/acc: 0.263 0.924, valid loss/acc: 0.278 0.917\n",
      "epoch 27 - train loss/acc: 0.261 0.924, valid loss/acc: 0.277 0.918\n",
      "epoch 28 - train loss/acc: 0.258 0.926, valid loss/acc: 0.273 0.920\n",
      "epoch 29 - train loss/acc: 0.255 0.927, valid loss/acc: 0.271 0.920\n",
      "epoch 30 - train loss/acc: 0.253 0.927, valid loss/acc: 0.269 0.920\n",
      "epoch 31 - train loss/acc: 0.250 0.928, valid loss/acc: 0.266 0.922\n",
      "epoch 32 - train loss/acc: 0.248 0.929, valid loss/acc: 0.265 0.922\n",
      "epoch 33 - train loss/acc: 0.245 0.930, valid loss/acc: 0.262 0.924\n",
      "epoch 34 - train loss/acc: 0.242 0.931, valid loss/acc: 0.259 0.925\n",
      "epoch 35 - train loss/acc: 0.240 0.931, valid loss/acc: 0.257 0.925\n",
      "epoch 36 - train loss/acc: 0.239 0.932, valid loss/acc: 0.255 0.927\n",
      "epoch 37 - train loss/acc: 0.235 0.933, valid loss/acc: 0.252 0.926\n",
      "epoch 38 - train loss/acc: 0.233 0.933, valid loss/acc: 0.251 0.926\n",
      "epoch 39 - train loss/acc: 0.231 0.934, valid loss/acc: 0.248 0.927\n",
      "epoch 40 - train loss/acc: 0.228 0.935, valid loss/acc: 0.246 0.928\n",
      "epoch 41 - train loss/acc: 0.226 0.935, valid loss/acc: 0.244 0.928\n",
      "epoch 42 - train loss/acc: 0.224 0.936, valid loss/acc: 0.241 0.929\n",
      "epoch 43 - train loss/acc: 0.222 0.936, valid loss/acc: 0.240 0.929\n",
      "epoch 44 - train loss/acc: 0.219 0.938, valid loss/acc: 0.237 0.931\n",
      "epoch 45 - train loss/acc: 0.217 0.938, valid loss/acc: 0.234 0.932\n",
      "epoch 46 - train loss/acc: 0.215 0.939, valid loss/acc: 0.232 0.932\n",
      "epoch 47 - train loss/acc: 0.213 0.940, valid loss/acc: 0.232 0.932\n",
      "epoch 48 - train loss/acc: 0.210 0.941, valid loss/acc: 0.229 0.933\n",
      "epoch 49 - train loss/acc: 0.208 0.941, valid loss/acc: 0.226 0.934\n",
      "epoch 50 - train loss/acc: 0.206 0.942, valid loss/acc: 0.224 0.934\n",
      "epoch 51 - train loss/acc: 0.205 0.942, valid loss/acc: 0.223 0.934\n",
      "epoch 52 - train loss/acc: 0.202 0.944, valid loss/acc: 0.220 0.936\n",
      "epoch 53 - train loss/acc: 0.200 0.943, valid loss/acc: 0.219 0.936\n",
      "epoch 54 - train loss/acc: 0.198 0.945, valid loss/acc: 0.216 0.938\n",
      "epoch 55 - train loss/acc: 0.196 0.945, valid loss/acc: 0.215 0.938\n",
      "epoch 56 - train loss/acc: 0.194 0.946, valid loss/acc: 0.212 0.938\n",
      "epoch 57 - train loss/acc: 0.192 0.946, valid loss/acc: 0.210 0.939\n",
      "epoch 58 - train loss/acc: 0.191 0.946, valid loss/acc: 0.209 0.940\n",
      "epoch 59 - train loss/acc: 0.189 0.946, valid loss/acc: 0.207 0.939\n",
      "epoch 60 - train loss/acc: 0.186 0.947, valid loss/acc: 0.205 0.939\n",
      "epoch 61 - train loss/acc: 0.185 0.948, valid loss/acc: 0.204 0.940\n",
      "epoch 62 - train loss/acc: 0.183 0.949, valid loss/acc: 0.202 0.941\n",
      "epoch 63 - train loss/acc: 0.181 0.949, valid loss/acc: 0.200 0.941\n",
      "epoch 64 - train loss/acc: 0.180 0.950, valid loss/acc: 0.199 0.943\n",
      "epoch 65 - train loss/acc: 0.178 0.950, valid loss/acc: 0.197 0.943\n",
      "epoch 66 - train loss/acc: 0.176 0.950, valid loss/acc: 0.196 0.943\n",
      "epoch 67 - train loss/acc: 0.174 0.951, valid loss/acc: 0.194 0.943\n",
      "epoch 68 - train loss/acc: 0.173 0.951, valid loss/acc: 0.193 0.943\n",
      "epoch 69 - train loss/acc: 0.171 0.952, valid loss/acc: 0.191 0.944\n",
      "epoch 70 - train loss/acc: 0.170 0.952, valid loss/acc: 0.190 0.945\n",
      "epoch 71 - train loss/acc: 0.169 0.952, valid loss/acc: 0.188 0.945\n",
      "epoch 72 - train loss/acc: 0.167 0.953, valid loss/acc: 0.186 0.945\n",
      "epoch 73 - train loss/acc: 0.165 0.954, valid loss/acc: 0.185 0.945\n",
      "epoch 74 - train loss/acc: 0.164 0.954, valid loss/acc: 0.185 0.945\n",
      "epoch 75 - train loss/acc: 0.162 0.955, valid loss/acc: 0.182 0.948\n",
      "epoch 76 - train loss/acc: 0.161 0.955, valid loss/acc: 0.181 0.947\n",
      "epoch 77 - train loss/acc: 0.159 0.955, valid loss/acc: 0.179 0.947\n",
      "epoch 78 - train loss/acc: 0.158 0.956, valid loss/acc: 0.178 0.947\n",
      "epoch 79 - train loss/acc: 0.157 0.956, valid loss/acc: 0.177 0.948\n",
      "epoch 80 - train loss/acc: 0.155 0.957, valid loss/acc: 0.176 0.948\n",
      "epoch 81 - train loss/acc: 0.154 0.957, valid loss/acc: 0.176 0.949\n",
      "epoch 82 - train loss/acc: 0.153 0.957, valid loss/acc: 0.175 0.948\n",
      "epoch 83 - train loss/acc: 0.151 0.958, valid loss/acc: 0.172 0.950\n",
      "epoch 84 - train loss/acc: 0.150 0.958, valid loss/acc: 0.171 0.950\n",
      "epoch 85 - train loss/acc: 0.149 0.958, valid loss/acc: 0.171 0.949\n",
      "epoch 86 - train loss/acc: 0.148 0.959, valid loss/acc: 0.169 0.950\n",
      "epoch 87 - train loss/acc: 0.147 0.960, valid loss/acc: 0.168 0.951\n",
      "epoch 88 - train loss/acc: 0.145 0.960, valid loss/acc: 0.166 0.950\n",
      "epoch 89 - train loss/acc: 0.144 0.960, valid loss/acc: 0.165 0.951\n",
      "epoch 90 - train loss/acc: 0.143 0.961, valid loss/acc: 0.164 0.950\n",
      "epoch 91 - train loss/acc: 0.142 0.961, valid loss/acc: 0.164 0.952\n",
      "epoch 92 - train loss/acc: 0.140 0.961, valid loss/acc: 0.162 0.952\n",
      "epoch 93 - train loss/acc: 0.139 0.961, valid loss/acc: 0.162 0.952\n",
      "epoch 94 - train loss/acc: 0.138 0.961, valid loss/acc: 0.161 0.953\n",
      "epoch 95 - train loss/acc: 0.137 0.962, valid loss/acc: 0.159 0.952\n",
      "epoch 96 - train loss/acc: 0.136 0.962, valid loss/acc: 0.159 0.953\n",
      "epoch 97 - train loss/acc: 0.135 0.962, valid loss/acc: 0.157 0.953\n",
      "epoch 98 - train loss/acc: 0.134 0.963, valid loss/acc: 0.156 0.953\n",
      "epoch 99 - train loss/acc: 0.133 0.963, valid loss/acc: 0.155 0.954\n",
      "epoch 100 - train loss/acc: 0.131 0.963, valid loss/acc: 0.154 0.954\n",
      "epoch 101 - train loss/acc: 0.131 0.963, valid loss/acc: 0.153 0.955\n",
      "epoch 102 - train loss/acc: 0.129 0.964, valid loss/acc: 0.152 0.954\n",
      "epoch 103 - train loss/acc: 0.129 0.964, valid loss/acc: 0.152 0.955\n",
      "epoch 104 - train loss/acc: 0.128 0.964, valid loss/acc: 0.151 0.955\n",
      "epoch 105 - train loss/acc: 0.126 0.965, valid loss/acc: 0.150 0.955\n",
      "epoch 106 - train loss/acc: 0.126 0.965, valid loss/acc: 0.149 0.956\n",
      "epoch 107 - train loss/acc: 0.125 0.965, valid loss/acc: 0.149 0.956\n",
      "epoch 108 - train loss/acc: 0.124 0.965, valid loss/acc: 0.147 0.956\n",
      "epoch 109 - train loss/acc: 0.123 0.966, valid loss/acc: 0.146 0.957\n",
      "epoch 110 - train loss/acc: 0.122 0.966, valid loss/acc: 0.146 0.956\n",
      "epoch 111 - train loss/acc: 0.121 0.966, valid loss/acc: 0.145 0.957\n",
      "epoch 112 - train loss/acc: 0.120 0.966, valid loss/acc: 0.144 0.957\n",
      "epoch 113 - train loss/acc: 0.120 0.967, valid loss/acc: 0.144 0.957\n",
      "epoch 114 - train loss/acc: 0.119 0.967, valid loss/acc: 0.142 0.958\n",
      "epoch 115 - train loss/acc: 0.118 0.967, valid loss/acc: 0.143 0.958\n",
      "epoch 116 - train loss/acc: 0.117 0.968, valid loss/acc: 0.141 0.957\n",
      "epoch 117 - train loss/acc: 0.116 0.968, valid loss/acc: 0.140 0.959\n",
      "epoch 118 - train loss/acc: 0.115 0.968, valid loss/acc: 0.140 0.958\n",
      "epoch 119 - train loss/acc: 0.114 0.969, valid loss/acc: 0.139 0.958\n",
      "epoch 120 - train loss/acc: 0.113 0.969, valid loss/acc: 0.138 0.959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 121 - train loss/acc: 0.113 0.969, valid loss/acc: 0.138 0.959\n",
      "epoch 122 - train loss/acc: 0.112 0.969, valid loss/acc: 0.137 0.960\n",
      "epoch 123 - train loss/acc: 0.111 0.970, valid loss/acc: 0.136 0.960\n",
      "epoch 124 - train loss/acc: 0.110 0.970, valid loss/acc: 0.135 0.959\n",
      "epoch 125 - train loss/acc: 0.109 0.970, valid loss/acc: 0.135 0.960\n",
      "epoch 126 - train loss/acc: 0.109 0.970, valid loss/acc: 0.134 0.960\n",
      "epoch 127 - train loss/acc: 0.108 0.970, valid loss/acc: 0.134 0.960\n",
      "epoch 128 - train loss/acc: 0.107 0.971, valid loss/acc: 0.133 0.960\n",
      "epoch 129 - train loss/acc: 0.106 0.971, valid loss/acc: 0.132 0.961\n",
      "epoch 130 - train loss/acc: 0.106 0.971, valid loss/acc: 0.132 0.961\n",
      "epoch 131 - train loss/acc: 0.105 0.971, valid loss/acc: 0.131 0.962\n",
      "epoch 132 - train loss/acc: 0.104 0.972, valid loss/acc: 0.130 0.961\n",
      "epoch 133 - train loss/acc: 0.104 0.971, valid loss/acc: 0.131 0.962\n",
      "epoch 134 - train loss/acc: 0.103 0.972, valid loss/acc: 0.129 0.962\n",
      "epoch 135 - train loss/acc: 0.103 0.972, valid loss/acc: 0.129 0.962\n",
      "epoch 136 - train loss/acc: 0.102 0.972, valid loss/acc: 0.128 0.962\n",
      "epoch 137 - train loss/acc: 0.101 0.973, valid loss/acc: 0.127 0.963\n",
      "epoch 138 - train loss/acc: 0.100 0.972, valid loss/acc: 0.127 0.963\n",
      "epoch 139 - train loss/acc: 0.100 0.973, valid loss/acc: 0.127 0.962\n",
      "epoch 140 - train loss/acc: 0.099 0.973, valid loss/acc: 0.126 0.963\n",
      "epoch 141 - train loss/acc: 0.098 0.973, valid loss/acc: 0.125 0.963\n",
      "epoch 142 - train loss/acc: 0.098 0.974, valid loss/acc: 0.125 0.963\n",
      "epoch 143 - train loss/acc: 0.097 0.974, valid loss/acc: 0.124 0.964\n",
      "epoch 144 - train loss/acc: 0.096 0.974, valid loss/acc: 0.124 0.964\n",
      "epoch 145 - train loss/acc: 0.096 0.974, valid loss/acc: 0.124 0.964\n",
      "epoch 146 - train loss/acc: 0.095 0.974, valid loss/acc: 0.123 0.964\n",
      "epoch 147 - train loss/acc: 0.095 0.974, valid loss/acc: 0.123 0.964\n",
      "epoch 148 - train loss/acc: 0.094 0.974, valid loss/acc: 0.121 0.965\n",
      "epoch 149 - train loss/acc: 0.094 0.975, valid loss/acc: 0.122 0.964\n",
      "epoch 150 - train loss/acc: 0.093 0.975, valid loss/acc: 0.121 0.965\n",
      "epoch 151 - train loss/acc: 0.092 0.975, valid loss/acc: 0.120 0.966\n",
      "epoch 152 - train loss/acc: 0.091 0.975, valid loss/acc: 0.120 0.965\n",
      "epoch 153 - train loss/acc: 0.091 0.975, valid loss/acc: 0.119 0.965\n",
      "epoch 154 - train loss/acc: 0.090 0.975, valid loss/acc: 0.119 0.965\n",
      "epoch 155 - train loss/acc: 0.090 0.975, valid loss/acc: 0.118 0.966\n",
      "epoch 156 - train loss/acc: 0.089 0.976, valid loss/acc: 0.118 0.966\n",
      "epoch 157 - train loss/acc: 0.089 0.976, valid loss/acc: 0.117 0.966\n",
      "epoch 158 - train loss/acc: 0.088 0.976, valid loss/acc: 0.117 0.966\n",
      "epoch 159 - train loss/acc: 0.087 0.976, valid loss/acc: 0.116 0.966\n",
      "epoch 160 - train loss/acc: 0.087 0.976, valid loss/acc: 0.116 0.966\n",
      "epoch 161 - train loss/acc: 0.087 0.976, valid loss/acc: 0.116 0.965\n",
      "epoch 162 - train loss/acc: 0.086 0.976, valid loss/acc: 0.115 0.967\n",
      "epoch 163 - train loss/acc: 0.085 0.976, valid loss/acc: 0.115 0.966\n",
      "epoch 164 - train loss/acc: 0.085 0.977, valid loss/acc: 0.115 0.966\n",
      "epoch 165 - train loss/acc: 0.084 0.977, valid loss/acc: 0.114 0.967\n",
      "epoch 166 - train loss/acc: 0.084 0.977, valid loss/acc: 0.114 0.966\n",
      "epoch 167 - train loss/acc: 0.083 0.977, valid loss/acc: 0.114 0.966\n",
      "epoch 168 - train loss/acc: 0.083 0.977, valid loss/acc: 0.113 0.966\n",
      "epoch 169 - train loss/acc: 0.082 0.977, valid loss/acc: 0.113 0.967\n",
      "epoch 170 - train loss/acc: 0.082 0.977, valid loss/acc: 0.113 0.966\n",
      "epoch 171 - train loss/acc: 0.081 0.978, valid loss/acc: 0.112 0.968\n",
      "epoch 172 - train loss/acc: 0.081 0.978, valid loss/acc: 0.111 0.967\n",
      "epoch 173 - train loss/acc: 0.081 0.978, valid loss/acc: 0.111 0.967\n",
      "epoch 174 - train loss/acc: 0.080 0.978, valid loss/acc: 0.111 0.967\n",
      "epoch 175 - train loss/acc: 0.080 0.978, valid loss/acc: 0.111 0.968\n",
      "epoch 176 - train loss/acc: 0.079 0.978, valid loss/acc: 0.110 0.967\n",
      "epoch 177 - train loss/acc: 0.079 0.978, valid loss/acc: 0.110 0.968\n",
      "epoch 178 - train loss/acc: 0.078 0.978, valid loss/acc: 0.109 0.968\n",
      "epoch 179 - train loss/acc: 0.078 0.979, valid loss/acc: 0.109 0.968\n",
      "epoch 180 - train loss/acc: 0.077 0.979, valid loss/acc: 0.109 0.968\n",
      "epoch 181 - train loss/acc: 0.077 0.979, valid loss/acc: 0.108 0.968\n",
      "epoch 182 - train loss/acc: 0.076 0.979, valid loss/acc: 0.108 0.968\n",
      "epoch 183 - train loss/acc: 0.076 0.979, valid loss/acc: 0.108 0.968\n",
      "epoch 184 - train loss/acc: 0.076 0.979, valid loss/acc: 0.107 0.969\n",
      "epoch 185 - train loss/acc: 0.075 0.979, valid loss/acc: 0.107 0.969\n",
      "epoch 186 - train loss/acc: 0.075 0.979, valid loss/acc: 0.107 0.969\n",
      "epoch 187 - train loss/acc: 0.074 0.980, valid loss/acc: 0.106 0.969\n",
      "epoch 188 - train loss/acc: 0.074 0.980, valid loss/acc: 0.106 0.969\n",
      "epoch 189 - train loss/acc: 0.073 0.980, valid loss/acc: 0.106 0.969\n",
      "epoch 190 - train loss/acc: 0.073 0.980, valid loss/acc: 0.105 0.969\n",
      "epoch 191 - train loss/acc: 0.072 0.980, valid loss/acc: 0.105 0.969\n",
      "epoch 192 - train loss/acc: 0.072 0.980, valid loss/acc: 0.105 0.969\n",
      "epoch 193 - train loss/acc: 0.072 0.980, valid loss/acc: 0.105 0.970\n",
      "epoch 194 - train loss/acc: 0.071 0.981, valid loss/acc: 0.104 0.970\n",
      "epoch 195 - train loss/acc: 0.071 0.981, valid loss/acc: 0.104 0.970\n",
      "epoch 196 - train loss/acc: 0.071 0.981, valid loss/acc: 0.104 0.969\n",
      "epoch 197 - train loss/acc: 0.070 0.981, valid loss/acc: 0.104 0.970\n",
      "epoch 198 - train loss/acc: 0.070 0.981, valid loss/acc: 0.103 0.970\n",
      "epoch 199 - train loss/acc: 0.070 0.981, valid loss/acc: 0.103 0.971\n",
      "epoch 200 - train loss/acc: 0.069 0.982, valid loss/acc: 0.103 0.970\n",
      "epoch 201 - train loss/acc: 0.069 0.981, valid loss/acc: 0.102 0.970\n",
      "epoch 202 - train loss/acc: 0.068 0.982, valid loss/acc: 0.102 0.970\n",
      "epoch 203 - train loss/acc: 0.068 0.982, valid loss/acc: 0.102 0.970\n",
      "epoch 204 - train loss/acc: 0.068 0.982, valid loss/acc: 0.102 0.971\n",
      "epoch 205 - train loss/acc: 0.067 0.982, valid loss/acc: 0.101 0.970\n",
      "epoch 206 - train loss/acc: 0.067 0.982, valid loss/acc: 0.101 0.971\n",
      "epoch 207 - train loss/acc: 0.067 0.982, valid loss/acc: 0.101 0.971\n",
      "epoch 208 - train loss/acc: 0.066 0.982, valid loss/acc: 0.101 0.971\n",
      "epoch 209 - train loss/acc: 0.066 0.982, valid loss/acc: 0.101 0.971\n",
      "epoch 210 - train loss/acc: 0.066 0.983, valid loss/acc: 0.100 0.971\n",
      "epoch 211 - train loss/acc: 0.065 0.983, valid loss/acc: 0.100 0.972\n",
      "epoch 212 - train loss/acc: 0.065 0.983, valid loss/acc: 0.100 0.971\n",
      "epoch 213 - train loss/acc: 0.065 0.983, valid loss/acc: 0.100 0.971\n",
      "epoch 214 - train loss/acc: 0.064 0.983, valid loss/acc: 0.099 0.971\n",
      "epoch 215 - train loss/acc: 0.064 0.983, valid loss/acc: 0.099 0.971\n",
      "epoch 216 - train loss/acc: 0.064 0.983, valid loss/acc: 0.099 0.971\n",
      "epoch 217 - train loss/acc: 0.063 0.983, valid loss/acc: 0.099 0.972\n",
      "epoch 218 - train loss/acc: 0.063 0.983, valid loss/acc: 0.098 0.971\n",
      "epoch 219 - train loss/acc: 0.063 0.984, valid loss/acc: 0.098 0.971\n",
      "epoch 220 - train loss/acc: 0.062 0.983, valid loss/acc: 0.098 0.971\n",
      "epoch 221 - train loss/acc: 0.062 0.984, valid loss/acc: 0.098 0.972\n",
      "epoch 222 - train loss/acc: 0.061 0.984, valid loss/acc: 0.097 0.971\n",
      "epoch 223 - train loss/acc: 0.061 0.984, valid loss/acc: 0.097 0.972\n",
      "epoch 224 - train loss/acc: 0.061 0.984, valid loss/acc: 0.097 0.972\n",
      "epoch 225 - train loss/acc: 0.060 0.984, valid loss/acc: 0.096 0.972\n",
      "epoch 226 - train loss/acc: 0.060 0.984, valid loss/acc: 0.097 0.972\n",
      "epoch 227 - train loss/acc: 0.060 0.985, valid loss/acc: 0.096 0.972\n",
      "epoch 228 - train loss/acc: 0.060 0.984, valid loss/acc: 0.096 0.972\n",
      "epoch 229 - train loss/acc: 0.059 0.985, valid loss/acc: 0.096 0.972\n",
      "epoch 230 - train loss/acc: 0.059 0.985, valid loss/acc: 0.095 0.973\n",
      "epoch 231 - train loss/acc: 0.059 0.985, valid loss/acc: 0.095 0.972\n",
      "epoch 232 - train loss/acc: 0.058 0.985, valid loss/acc: 0.096 0.972\n",
      "epoch 233 - train loss/acc: 0.058 0.985, valid loss/acc: 0.095 0.972\n",
      "epoch 234 - train loss/acc: 0.058 0.985, valid loss/acc: 0.096 0.972\n",
      "epoch 235 - train loss/acc: 0.058 0.985, valid loss/acc: 0.095 0.973\n",
      "epoch 236 - train loss/acc: 0.057 0.985, valid loss/acc: 0.094 0.972\n",
      "epoch 237 - train loss/acc: 0.057 0.985, valid loss/acc: 0.094 0.973\n",
      "epoch 238 - train loss/acc: 0.057 0.985, valid loss/acc: 0.094 0.973\n",
      "epoch 239 - train loss/acc: 0.056 0.986, valid loss/acc: 0.094 0.973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 240 - train loss/acc: 0.056 0.986, valid loss/acc: 0.094 0.973\n",
      "epoch 241 - train loss/acc: 0.056 0.986, valid loss/acc: 0.093 0.973\n",
      "epoch 242 - train loss/acc: 0.055 0.986, valid loss/acc: 0.093 0.973\n",
      "epoch 243 - train loss/acc: 0.055 0.986, valid loss/acc: 0.093 0.973\n",
      "epoch 244 - train loss/acc: 0.055 0.986, valid loss/acc: 0.093 0.973\n",
      "epoch 245 - train loss/acc: 0.055 0.986, valid loss/acc: 0.093 0.973\n",
      "epoch 246 - train loss/acc: 0.054 0.986, valid loss/acc: 0.093 0.973\n",
      "epoch 247 - train loss/acc: 0.054 0.986, valid loss/acc: 0.092 0.974\n",
      "epoch 248 - train loss/acc: 0.054 0.986, valid loss/acc: 0.092 0.973\n",
      "epoch 249 - train loss/acc: 0.054 0.986, valid loss/acc: 0.092 0.973\n",
      "epoch 250 - train loss/acc: 0.053 0.986, valid loss/acc: 0.092 0.973\n",
      "epoch 251 - train loss/acc: 0.053 0.986, valid loss/acc: 0.092 0.974\n",
      "epoch 252 - train loss/acc: 0.053 0.987, valid loss/acc: 0.092 0.974\n",
      "epoch 253 - train loss/acc: 0.053 0.987, valid loss/acc: 0.092 0.974\n",
      "epoch 254 - train loss/acc: 0.052 0.987, valid loss/acc: 0.091 0.974\n",
      "epoch 255 - train loss/acc: 0.052 0.987, valid loss/acc: 0.091 0.974\n",
      "epoch 256 - train loss/acc: 0.052 0.987, valid loss/acc: 0.091 0.974\n",
      "epoch 257 - train loss/acc: 0.052 0.987, valid loss/acc: 0.091 0.974\n",
      "epoch 258 - train loss/acc: 0.051 0.987, valid loss/acc: 0.091 0.973\n",
      "epoch 259 - train loss/acc: 0.051 0.987, valid loss/acc: 0.090 0.974\n",
      "epoch 260 - train loss/acc: 0.051 0.987, valid loss/acc: 0.090 0.974\n",
      "epoch 261 - train loss/acc: 0.051 0.987, valid loss/acc: 0.090 0.974\n",
      "epoch 262 - train loss/acc: 0.050 0.988, valid loss/acc: 0.090 0.975\n",
      "epoch 263 - train loss/acc: 0.050 0.987, valid loss/acc: 0.090 0.974\n",
      "epoch 264 - train loss/acc: 0.050 0.988, valid loss/acc: 0.089 0.974\n",
      "epoch 265 - train loss/acc: 0.050 0.988, valid loss/acc: 0.090 0.974\n",
      "epoch 266 - train loss/acc: 0.049 0.988, valid loss/acc: 0.089 0.974\n",
      "epoch 267 - train loss/acc: 0.049 0.988, valid loss/acc: 0.089 0.975\n",
      "epoch 268 - train loss/acc: 0.049 0.988, valid loss/acc: 0.089 0.974\n",
      "epoch 269 - train loss/acc: 0.049 0.988, valid loss/acc: 0.089 0.974\n",
      "epoch 270 - train loss/acc: 0.048 0.988, valid loss/acc: 0.089 0.974\n",
      "epoch 271 - train loss/acc: 0.048 0.988, valid loss/acc: 0.089 0.974\n",
      "epoch 272 - train loss/acc: 0.048 0.988, valid loss/acc: 0.088 0.974\n",
      "epoch 273 - train loss/acc: 0.048 0.988, valid loss/acc: 0.088 0.975\n",
      "epoch 274 - train loss/acc: 0.047 0.988, valid loss/acc: 0.088 0.974\n",
      "epoch 275 - train loss/acc: 0.047 0.988, valid loss/acc: 0.088 0.975\n",
      "epoch 276 - train loss/acc: 0.047 0.988, valid loss/acc: 0.088 0.975\n",
      "epoch 277 - train loss/acc: 0.047 0.988, valid loss/acc: 0.088 0.974\n",
      "epoch 278 - train loss/acc: 0.047 0.988, valid loss/acc: 0.088 0.974\n",
      "epoch 279 - train loss/acc: 0.046 0.989, valid loss/acc: 0.087 0.975\n",
      "epoch 280 - train loss/acc: 0.046 0.989, valid loss/acc: 0.088 0.974\n",
      "epoch 281 - train loss/acc: 0.046 0.989, valid loss/acc: 0.087 0.975\n",
      "epoch 282 - train loss/acc: 0.046 0.989, valid loss/acc: 0.087 0.975\n",
      "epoch 283 - train loss/acc: 0.046 0.989, valid loss/acc: 0.087 0.975\n",
      "epoch 284 - train loss/acc: 0.045 0.989, valid loss/acc: 0.087 0.975\n",
      "epoch 285 - train loss/acc: 0.045 0.989, valid loss/acc: 0.087 0.975\n",
      "epoch 286 - train loss/acc: 0.045 0.989, valid loss/acc: 0.086 0.975\n",
      "epoch 287 - train loss/acc: 0.045 0.989, valid loss/acc: 0.087 0.975\n",
      "epoch 288 - train loss/acc: 0.044 0.989, valid loss/acc: 0.086 0.975\n",
      "epoch 289 - train loss/acc: 0.044 0.989, valid loss/acc: 0.086 0.974\n",
      "epoch 290 - train loss/acc: 0.044 0.989, valid loss/acc: 0.086 0.975\n",
      "epoch 291 - train loss/acc: 0.044 0.989, valid loss/acc: 0.086 0.975\n",
      "epoch 292 - train loss/acc: 0.044 0.989, valid loss/acc: 0.086 0.975\n",
      "epoch 293 - train loss/acc: 0.043 0.990, valid loss/acc: 0.086 0.975\n",
      "epoch 294 - train loss/acc: 0.043 0.989, valid loss/acc: 0.086 0.975\n",
      "epoch 295 - train loss/acc: 0.043 0.990, valid loss/acc: 0.086 0.975\n",
      "epoch 296 - train loss/acc: 0.043 0.990, valid loss/acc: 0.085 0.975\n",
      "epoch 297 - train loss/acc: 0.043 0.989, valid loss/acc: 0.085 0.975\n",
      "epoch 298 - train loss/acc: 0.042 0.990, valid loss/acc: 0.085 0.975\n",
      "epoch 299 - train loss/acc: 0.042 0.990, valid loss/acc: 0.085 0.975\n",
      "epoch 300 - train loss/acc: 0.042 0.990, valid loss/acc: 0.085 0.975\n",
      "epoch 301 - train loss/acc: 0.042 0.990, valid loss/acc: 0.085 0.975\n",
      "epoch 302 - train loss/acc: 0.042 0.990, valid loss/acc: 0.085 0.976\n",
      "epoch 303 - train loss/acc: 0.041 0.990, valid loss/acc: 0.085 0.975\n",
      "epoch 304 - train loss/acc: 0.041 0.990, valid loss/acc: 0.085 0.975\n",
      "epoch 305 - train loss/acc: 0.041 0.990, valid loss/acc: 0.085 0.975\n",
      "epoch 306 - train loss/acc: 0.041 0.990, valid loss/acc: 0.085 0.975\n",
      "epoch 307 - train loss/acc: 0.041 0.990, valid loss/acc: 0.085 0.975\n",
      "epoch 308 - train loss/acc: 0.041 0.991, valid loss/acc: 0.084 0.975\n",
      "epoch 309 - train loss/acc: 0.040 0.991, valid loss/acc: 0.084 0.975\n",
      "epoch 310 - train loss/acc: 0.040 0.991, valid loss/acc: 0.084 0.975\n",
      "epoch 311 - train loss/acc: 0.040 0.991, valid loss/acc: 0.084 0.975\n",
      "epoch 312 - train loss/acc: 0.040 0.991, valid loss/acc: 0.084 0.975\n",
      "epoch 313 - train loss/acc: 0.040 0.991, valid loss/acc: 0.084 0.976\n",
      "epoch 314 - train loss/acc: 0.040 0.991, valid loss/acc: 0.084 0.976\n",
      "epoch 315 - train loss/acc: 0.039 0.991, valid loss/acc: 0.084 0.976\n",
      "epoch 316 - train loss/acc: 0.039 0.991, valid loss/acc: 0.083 0.975\n",
      "epoch 317 - train loss/acc: 0.039 0.991, valid loss/acc: 0.083 0.976\n",
      "epoch 318 - train loss/acc: 0.039 0.991, valid loss/acc: 0.083 0.975\n",
      "epoch 319 - train loss/acc: 0.039 0.991, valid loss/acc: 0.083 0.976\n",
      "epoch 320 - train loss/acc: 0.038 0.991, valid loss/acc: 0.083 0.976\n",
      "epoch 321 - train loss/acc: 0.038 0.991, valid loss/acc: 0.083 0.976\n",
      "epoch 322 - train loss/acc: 0.038 0.991, valid loss/acc: 0.083 0.976\n",
      "epoch 323 - train loss/acc: 0.038 0.991, valid loss/acc: 0.083 0.976\n",
      "epoch 324 - train loss/acc: 0.038 0.992, valid loss/acc: 0.083 0.976\n",
      "epoch 325 - train loss/acc: 0.038 0.991, valid loss/acc: 0.083 0.976\n",
      "epoch 326 - train loss/acc: 0.037 0.992, valid loss/acc: 0.083 0.976\n",
      "epoch 327 - train loss/acc: 0.037 0.992, valid loss/acc: 0.082 0.976\n",
      "epoch 328 - train loss/acc: 0.037 0.992, valid loss/acc: 0.082 0.976\n",
      "epoch 329 - train loss/acc: 0.037 0.992, valid loss/acc: 0.082 0.976\n",
      "epoch 330 - train loss/acc: 0.037 0.992, valid loss/acc: 0.082 0.976\n",
      "epoch 331 - train loss/acc: 0.037 0.992, valid loss/acc: 0.082 0.976\n",
      "epoch 332 - train loss/acc: 0.036 0.992, valid loss/acc: 0.082 0.976\n",
      "epoch 333 - train loss/acc: 0.036 0.992, valid loss/acc: 0.082 0.976\n",
      "epoch 334 - train loss/acc: 0.036 0.992, valid loss/acc: 0.082 0.976\n",
      "epoch 335 - train loss/acc: 0.036 0.992, valid loss/acc: 0.082 0.976\n",
      "epoch 336 - train loss/acc: 0.036 0.992, valid loss/acc: 0.082 0.977\n",
      "epoch 337 - train loss/acc: 0.036 0.992, valid loss/acc: 0.082 0.976\n",
      "epoch 338 - train loss/acc: 0.035 0.992, valid loss/acc: 0.082 0.976\n",
      "epoch 339 - train loss/acc: 0.035 0.992, valid loss/acc: 0.082 0.976\n",
      "epoch 340 - train loss/acc: 0.035 0.992, valid loss/acc: 0.082 0.977\n",
      "epoch 341 - train loss/acc: 0.035 0.992, valid loss/acc: 0.081 0.977\n",
      "epoch 342 - train loss/acc: 0.035 0.993, valid loss/acc: 0.081 0.976\n",
      "epoch 343 - train loss/acc: 0.035 0.992, valid loss/acc: 0.081 0.976\n",
      "epoch 344 - train loss/acc: 0.035 0.993, valid loss/acc: 0.081 0.976\n",
      "epoch 345 - train loss/acc: 0.034 0.993, valid loss/acc: 0.081 0.977\n",
      "epoch 346 - train loss/acc: 0.034 0.993, valid loss/acc: 0.081 0.977\n",
      "epoch 347 - train loss/acc: 0.034 0.993, valid loss/acc: 0.081 0.977\n",
      "epoch 348 - train loss/acc: 0.034 0.993, valid loss/acc: 0.081 0.977\n",
      "epoch 349 - train loss/acc: 0.034 0.993, valid loss/acc: 0.081 0.977\n",
      "epoch 350 - train loss/acc: 0.034 0.993, valid loss/acc: 0.081 0.976\n",
      "epoch 351 - train loss/acc: 0.034 0.993, valid loss/acc: 0.081 0.977\n",
      "epoch 352 - train loss/acc: 0.033 0.993, valid loss/acc: 0.081 0.977\n",
      "epoch 353 - train loss/acc: 0.033 0.993, valid loss/acc: 0.081 0.977\n",
      "epoch 354 - train loss/acc: 0.033 0.993, valid loss/acc: 0.081 0.977\n",
      "epoch 355 - train loss/acc: 0.033 0.993, valid loss/acc: 0.080 0.976\n",
      "epoch 356 - train loss/acc: 0.033 0.993, valid loss/acc: 0.080 0.977\n",
      "epoch 357 - train loss/acc: 0.033 0.993, valid loss/acc: 0.080 0.977\n",
      "epoch 358 - train loss/acc: 0.033 0.993, valid loss/acc: 0.080 0.977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 359 - train loss/acc: 0.032 0.994, valid loss/acc: 0.080 0.977\n",
      "epoch 360 - train loss/acc: 0.032 0.993, valid loss/acc: 0.080 0.977\n",
      "epoch 361 - train loss/acc: 0.032 0.994, valid loss/acc: 0.080 0.977\n",
      "epoch 362 - train loss/acc: 0.032 0.994, valid loss/acc: 0.080 0.977\n",
      "epoch 363 - train loss/acc: 0.032 0.993, valid loss/acc: 0.080 0.977\n",
      "epoch 364 - train loss/acc: 0.032 0.994, valid loss/acc: 0.080 0.977\n",
      "epoch 365 - train loss/acc: 0.032 0.994, valid loss/acc: 0.080 0.977\n",
      "epoch 366 - train loss/acc: 0.032 0.994, valid loss/acc: 0.080 0.977\n",
      "epoch 367 - train loss/acc: 0.031 0.994, valid loss/acc: 0.080 0.977\n",
      "epoch 368 - train loss/acc: 0.031 0.994, valid loss/acc: 0.080 0.977\n",
      "epoch 369 - train loss/acc: 0.031 0.994, valid loss/acc: 0.079 0.977\n",
      "epoch 370 - train loss/acc: 0.031 0.994, valid loss/acc: 0.080 0.977\n",
      "epoch 371 - train loss/acc: 0.031 0.994, valid loss/acc: 0.079 0.977\n",
      "epoch 372 - train loss/acc: 0.031 0.994, valid loss/acc: 0.080 0.977\n",
      "epoch 373 - train loss/acc: 0.031 0.994, valid loss/acc: 0.079 0.977\n",
      "epoch 374 - train loss/acc: 0.030 0.994, valid loss/acc: 0.079 0.978\n",
      "epoch 375 - train loss/acc: 0.030 0.994, valid loss/acc: 0.079 0.978\n",
      "epoch 376 - train loss/acc: 0.030 0.994, valid loss/acc: 0.079 0.977\n",
      "epoch 377 - train loss/acc: 0.030 0.994, valid loss/acc: 0.079 0.978\n",
      "epoch 378 - train loss/acc: 0.030 0.994, valid loss/acc: 0.079 0.977\n",
      "epoch 379 - train loss/acc: 0.030 0.994, valid loss/acc: 0.079 0.978\n",
      "epoch 380 - train loss/acc: 0.030 0.994, valid loss/acc: 0.079 0.978\n",
      "epoch 381 - train loss/acc: 0.030 0.994, valid loss/acc: 0.079 0.978\n",
      "epoch 382 - train loss/acc: 0.030 0.994, valid loss/acc: 0.079 0.977\n",
      "epoch 383 - train loss/acc: 0.029 0.995, valid loss/acc: 0.079 0.977\n",
      "epoch 384 - train loss/acc: 0.029 0.994, valid loss/acc: 0.079 0.978\n",
      "epoch 385 - train loss/acc: 0.029 0.995, valid loss/acc: 0.079 0.978\n",
      "epoch 386 - train loss/acc: 0.029 0.995, valid loss/acc: 0.079 0.977\n",
      "epoch 387 - train loss/acc: 0.029 0.995, valid loss/acc: 0.078 0.978\n",
      "epoch 388 - train loss/acc: 0.029 0.995, valid loss/acc: 0.079 0.977\n",
      "epoch 389 - train loss/acc: 0.029 0.995, valid loss/acc: 0.079 0.978\n",
      "epoch 390 - train loss/acc: 0.029 0.995, valid loss/acc: 0.078 0.978\n",
      "epoch 391 - train loss/acc: 0.028 0.995, valid loss/acc: 0.078 0.978\n",
      "epoch 392 - train loss/acc: 0.028 0.995, valid loss/acc: 0.078 0.977\n",
      "epoch 393 - train loss/acc: 0.028 0.995, valid loss/acc: 0.078 0.977\n",
      "epoch 394 - train loss/acc: 0.028 0.995, valid loss/acc: 0.078 0.978\n",
      "epoch 395 - train loss/acc: 0.028 0.995, valid loss/acc: 0.078 0.978\n",
      "epoch 396 - train loss/acc: 0.028 0.995, valid loss/acc: 0.078 0.978\n",
      "epoch 397 - train loss/acc: 0.028 0.995, valid loss/acc: 0.078 0.977\n",
      "epoch 398 - train loss/acc: 0.028 0.995, valid loss/acc: 0.078 0.978\n",
      "epoch 399 - train loss/acc: 0.028 0.995, valid loss/acc: 0.078 0.978\n",
      "epoch 400 - train loss/acc: 0.027 0.995, valid loss/acc: 0.078 0.978\n",
      "epoch 401 - train loss/acc: 0.027 0.995, valid loss/acc: 0.078 0.978\n",
      "epoch 402 - train loss/acc: 0.027 0.995, valid loss/acc: 0.078 0.978\n",
      "epoch 403 - train loss/acc: 0.027 0.995, valid loss/acc: 0.078 0.978\n",
      "epoch 404 - train loss/acc: 0.027 0.995, valid loss/acc: 0.078 0.978\n",
      "epoch 405 - train loss/acc: 0.027 0.995, valid loss/acc: 0.078 0.978\n",
      "epoch 406 - train loss/acc: 0.027 0.995, valid loss/acc: 0.078 0.978\n",
      "epoch 407 - train loss/acc: 0.027 0.995, valid loss/acc: 0.078 0.977\n",
      "epoch 408 - train loss/acc: 0.026 0.995, valid loss/acc: 0.078 0.978\n",
      "epoch 409 - train loss/acc: 0.026 0.995, valid loss/acc: 0.077 0.978\n",
      "epoch 410 - train loss/acc: 0.026 0.995, valid loss/acc: 0.078 0.978\n",
      "epoch 411 - train loss/acc: 0.026 0.995, valid loss/acc: 0.078 0.977\n",
      "epoch 412 - train loss/acc: 0.026 0.995, valid loss/acc: 0.077 0.978\n",
      "epoch 413 - train loss/acc: 0.026 0.996, valid loss/acc: 0.077 0.978\n",
      "epoch 414 - train loss/acc: 0.026 0.995, valid loss/acc: 0.077 0.978\n",
      "epoch 415 - train loss/acc: 0.026 0.996, valid loss/acc: 0.077 0.977\n",
      "epoch 416 - train loss/acc: 0.026 0.996, valid loss/acc: 0.077 0.978\n",
      "epoch 417 - train loss/acc: 0.026 0.996, valid loss/acc: 0.077 0.979\n",
      "epoch 418 - train loss/acc: 0.026 0.995, valid loss/acc: 0.077 0.978\n",
      "epoch 419 - train loss/acc: 0.025 0.996, valid loss/acc: 0.077 0.978\n",
      "epoch 420 - train loss/acc: 0.025 0.996, valid loss/acc: 0.077 0.978\n",
      "epoch 421 - train loss/acc: 0.025 0.996, valid loss/acc: 0.077 0.978\n",
      "epoch 422 - train loss/acc: 0.025 0.996, valid loss/acc: 0.077 0.978\n",
      "epoch 423 - train loss/acc: 0.025 0.996, valid loss/acc: 0.077 0.978\n",
      "epoch 424 - train loss/acc: 0.025 0.996, valid loss/acc: 0.077 0.978\n",
      "epoch 425 - train loss/acc: 0.025 0.996, valid loss/acc: 0.077 0.978\n",
      "epoch 426 - train loss/acc: 0.025 0.996, valid loss/acc: 0.077 0.978\n",
      "epoch 427 - train loss/acc: 0.025 0.996, valid loss/acc: 0.077 0.978\n",
      "epoch 428 - train loss/acc: 0.025 0.996, valid loss/acc: 0.077 0.978\n",
      "epoch 429 - train loss/acc: 0.024 0.996, valid loss/acc: 0.077 0.978\n",
      "epoch 430 - train loss/acc: 0.024 0.996, valid loss/acc: 0.077 0.978\n",
      "epoch 431 - train loss/acc: 0.024 0.996, valid loss/acc: 0.076 0.978\n",
      "epoch 432 - train loss/acc: 0.024 0.996, valid loss/acc: 0.077 0.978\n",
      "epoch 433 - train loss/acc: 0.024 0.996, valid loss/acc: 0.077 0.978\n",
      "epoch 434 - train loss/acc: 0.024 0.996, valid loss/acc: 0.077 0.978\n",
      "epoch 435 - train loss/acc: 0.024 0.996, valid loss/acc: 0.077 0.978\n",
      "epoch 436 - train loss/acc: 0.024 0.996, valid loss/acc: 0.076 0.978\n",
      "epoch 437 - train loss/acc: 0.024 0.996, valid loss/acc: 0.076 0.978\n",
      "epoch 438 - train loss/acc: 0.024 0.996, valid loss/acc: 0.077 0.978\n",
      "epoch 439 - train loss/acc: 0.023 0.996, valid loss/acc: 0.076 0.978\n",
      "epoch 440 - train loss/acc: 0.023 0.996, valid loss/acc: 0.076 0.978\n",
      "epoch 441 - train loss/acc: 0.023 0.996, valid loss/acc: 0.076 0.978\n",
      "epoch 442 - train loss/acc: 0.023 0.996, valid loss/acc: 0.076 0.978\n",
      "epoch 443 - train loss/acc: 0.023 0.996, valid loss/acc: 0.076 0.978\n",
      "epoch 444 - train loss/acc: 0.023 0.996, valid loss/acc: 0.076 0.978\n",
      "epoch 445 - train loss/acc: 0.023 0.996, valid loss/acc: 0.076 0.978\n",
      "epoch 446 - train loss/acc: 0.023 0.996, valid loss/acc: 0.076 0.978\n",
      "epoch 447 - train loss/acc: 0.023 0.996, valid loss/acc: 0.076 0.978\n",
      "epoch 448 - train loss/acc: 0.023 0.996, valid loss/acc: 0.076 0.978\n",
      "epoch 449 - train loss/acc: 0.023 0.996, valid loss/acc: 0.076 0.978\n",
      "epoch 450 - train loss/acc: 0.022 0.996, valid loss/acc: 0.076 0.978\n",
      "epoch 451 - train loss/acc: 0.022 0.996, valid loss/acc: 0.076 0.978\n",
      "epoch 452 - train loss/acc: 0.022 0.996, valid loss/acc: 0.076 0.978\n",
      "epoch 453 - train loss/acc: 0.022 0.996, valid loss/acc: 0.076 0.978\n",
      "epoch 454 - train loss/acc: 0.022 0.997, valid loss/acc: 0.076 0.978\n",
      "epoch 455 - train loss/acc: 0.022 0.996, valid loss/acc: 0.076 0.978\n",
      "epoch 456 - train loss/acc: 0.022 0.997, valid loss/acc: 0.076 0.978\n",
      "epoch 457 - train loss/acc: 0.022 0.997, valid loss/acc: 0.076 0.978\n",
      "epoch 458 - train loss/acc: 0.022 0.997, valid loss/acc: 0.076 0.978\n",
      "epoch 459 - train loss/acc: 0.022 0.997, valid loss/acc: 0.076 0.978\n",
      "epoch 460 - train loss/acc: 0.022 0.997, valid loss/acc: 0.076 0.978\n",
      "epoch 461 - train loss/acc: 0.022 0.997, valid loss/acc: 0.076 0.978\n",
      "epoch 462 - train loss/acc: 0.022 0.997, valid loss/acc: 0.076 0.978\n",
      "epoch 463 - train loss/acc: 0.021 0.997, valid loss/acc: 0.076 0.978\n",
      "epoch 464 - train loss/acc: 0.021 0.997, valid loss/acc: 0.076 0.978\n",
      "epoch 465 - train loss/acc: 0.021 0.997, valid loss/acc: 0.076 0.978\n",
      "epoch 466 - train loss/acc: 0.021 0.997, valid loss/acc: 0.076 0.978\n",
      "epoch 467 - train loss/acc: 0.021 0.997, valid loss/acc: 0.076 0.978\n",
      "epoch 468 - train loss/acc: 0.021 0.997, valid loss/acc: 0.076 0.978\n",
      "epoch 469 - train loss/acc: 0.021 0.997, valid loss/acc: 0.076 0.978\n",
      "epoch 470 - train loss/acc: 0.021 0.997, valid loss/acc: 0.076 0.978\n",
      "epoch 471 - train loss/acc: 0.021 0.997, valid loss/acc: 0.076 0.978\n",
      "epoch 472 - train loss/acc: 0.021 0.997, valid loss/acc: 0.076 0.978\n",
      "epoch 473 - train loss/acc: 0.021 0.997, valid loss/acc: 0.075 0.978\n",
      "epoch 474 - train loss/acc: 0.021 0.997, valid loss/acc: 0.076 0.978\n",
      "epoch 475 - train loss/acc: 0.021 0.997, valid loss/acc: 0.075 0.978\n",
      "epoch 476 - train loss/acc: 0.021 0.997, valid loss/acc: 0.075 0.978\n",
      "epoch 477 - train loss/acc: 0.020 0.997, valid loss/acc: 0.076 0.978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 478 - train loss/acc: 0.020 0.997, valid loss/acc: 0.076 0.978\n",
      "epoch 479 - train loss/acc: 0.020 0.997, valid loss/acc: 0.075 0.978\n",
      "epoch 480 - train loss/acc: 0.020 0.997, valid loss/acc: 0.075 0.978\n",
      "epoch 481 - train loss/acc: 0.020 0.997, valid loss/acc: 0.076 0.977\n",
      "epoch 482 - train loss/acc: 0.020 0.997, valid loss/acc: 0.075 0.978\n",
      "epoch 483 - train loss/acc: 0.020 0.997, valid loss/acc: 0.075 0.978\n",
      "epoch 484 - train loss/acc: 0.020 0.997, valid loss/acc: 0.075 0.978\n",
      "epoch 485 - train loss/acc: 0.020 0.997, valid loss/acc: 0.075 0.978\n",
      "epoch 486 - train loss/acc: 0.020 0.997, valid loss/acc: 0.075 0.978\n",
      "epoch 487 - train loss/acc: 0.020 0.997, valid loss/acc: 0.075 0.978\n",
      "epoch 488 - train loss/acc: 0.020 0.997, valid loss/acc: 0.075 0.978\n",
      "epoch 489 - train loss/acc: 0.019 0.997, valid loss/acc: 0.075 0.978\n",
      "epoch 490 - train loss/acc: 0.019 0.997, valid loss/acc: 0.075 0.978\n",
      "epoch 491 - train loss/acc: 0.019 0.997, valid loss/acc: 0.075 0.978\n",
      "epoch 492 - train loss/acc: 0.019 0.998, valid loss/acc: 0.075 0.978\n",
      "epoch 493 - train loss/acc: 0.019 0.997, valid loss/acc: 0.075 0.978\n",
      "epoch 494 - train loss/acc: 0.019 0.997, valid loss/acc: 0.075 0.978\n",
      "epoch 495 - train loss/acc: 0.019 0.998, valid loss/acc: 0.075 0.978\n",
      "epoch 496 - train loss/acc: 0.019 0.997, valid loss/acc: 0.075 0.978\n",
      "epoch 497 - train loss/acc: 0.019 0.997, valid loss/acc: 0.075 0.978\n",
      "epoch 498 - train loss/acc: 0.019 0.998, valid loss/acc: 0.075 0.978\n",
      "epoch 499 - train loss/acc: 0.019 0.997, valid loss/acc: 0.075 0.978\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "lr, n_epochs, batch_size = 0.1, 500, 256\n",
    "model.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 435,
     "status": "ok",
     "timestamp": 1652428544682,
     "user": {
      "displayName": "­송종현 / 학생 / 데이터사이언스학과",
      "userId": "03940744101866465560"
     },
     "user_tz": -540
    },
    "id": "hpPsAlXU0T_Z",
    "outputId": "d1c231b5-f32e-4c09-82d1-e329e502b7ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test loss = 0.067, acc = 0.978\n"
     ]
    }
   ],
   "source": [
    "# evalute the model on test data\n",
    "Y_hat, _ = model.forward(X_test)\n",
    "test_loss = model.compute_loss(Y_test, Y_hat)\n",
    "test_acc = model.evaluate(Y_test, Y_hat)\n",
    "print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hh1PZpk_g0I"
   },
   "source": [
    "# Extra Credit (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0R0n6y9_AgXc"
   },
   "outputs": [],
   "source": [
    "def Relu(X):\n",
    "    return X*(X>0)\n",
    "def Relu_diffential(X):\n",
    "    return 1*(X>0)\n",
    "def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n",
    "    \"\"\"\n",
    "    initializes parameters with He Initialization.\n",
    "\n",
    "    Question (e)\n",
    "    - refer to https://paperswithcode.com/method/he-initialization for He initialization\n",
    "\n",
    "    Inputs\n",
    "    - input_dim\n",
    "    - num_hiddens\n",
    "    - num_classes\n",
    "    Returns\n",
    "    - params: a dictionary with the initialized parameters.\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "    params[\"W1\"]=np.random.normal(0, 1/np.sqrt(input_dim/2),(input_dim,num_hiddens))\n",
    "    params[\"W2\"]=np.random.normal(0, 1/np.sqrt(num_hiddens/2),(num_hiddens,num_classes))\n",
    "    params[\"b1\"]=np.zeros(num_hiddens)\n",
    "    params[\"b2\"]=np.zeros(num_classes)\n",
    "\n",
    "    return params\n",
    "\n",
    "def forward_relu(self, X):\n",
    "    \"\"\"\n",
    "    Defines and performs the feed forward step of a two-layer neural network.\n",
    "    Specifically, the network structue is given by\n",
    "\n",
    "        y = softmax(relu(X W1 + b1) W2 + b2)\n",
    "\n",
    "    where X is the input matrix of shape (N, D), y is the class distribution matrix\n",
    "    of shape (N, C), N is the number of examples (either the entire dataset or\n",
    "    a mini-batch), D is the feature dimensionality, and C is the number of classes.\n",
    "\n",
    "    Question (e)\n",
    "\n",
    "    Inputs\n",
    "        X: the input matrix of shape (N, D)\n",
    "\n",
    "    Returns\n",
    "        y: the output of the model\n",
    "        ff_dict: a dictionary containing all the fully connected units and activations.\n",
    "    \"\"\"\n",
    "    ff_dict = {}\n",
    "    ff_dict=self.params\n",
    "\n",
    "    ff_dict=self.params\n",
    "    ff_dict[\"h\"]=Relu((np.matmul(X,ff_dict[\"W1\"])+ff_dict[\"b1\"]))\n",
    "    y=softmax(np.matmul(ff_dict[\"h\"],ff_dict[\"W2\"])+ff_dict[\"b2\"])\n",
    "    ff_dict[\"y\"]=y\n",
    "\n",
    "    return y, ff_dict\n",
    "\n",
    "def backward_relu(self, X, Y, ff_dict):\n",
    "    \"\"\"\n",
    "    Performs backpropagation over the two-layer neural network, and returns\n",
    "    a dictionary of gradients of all model parameters.\n",
    "\n",
    "    Question (e)\n",
    "\n",
    "    Inputs:\n",
    "        - X: the input matrix of shape (B, D), where B is the number of examples\n",
    "            in a mini-batch, D is the feature dimensionality.\n",
    "        - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n",
    "            where B is the number of examples in a mini-batch, C is the number\n",
    "            of classes.\n",
    "        - ff_dict: the dictionary containing all the fully connected units and\n",
    "            activations.\n",
    "\n",
    "    Returns:\n",
    "        - grads: a dictionary containing the gradients of corresponding weights\n",
    "            and biases.\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    grads[\"h\"]=np.matmul(ff_dict[\"y\"]-Y,np.transpose(ff_dict[\"W2\"]))\n",
    "    grads[\"dW2\"]=np.matmul((np.transpose(ff_dict[\"h\"])),ff_dict[\"y\"]-Y)\n",
    "    grads[\"db1\"] =grads[\"h\"]*Relu_diffential(X)\n",
    "    grads[\"db1\"]=grads[\"db1\"].sum(0)\n",
    "    grads[\"dW1\"] = np.matmul(np.transpose(X),Relu_diffential(X)*grads[\"h\"])\n",
    "            # grads[\"dW1\"] = np.matmul(np.transpose(X),grads[\"h\"]*ff_dict[\"h\"]*ff_dict[\"minus h\"])\n",
    "    grads[\"db2\"] = (ff_dict[\"y\"]-Y)\n",
    "    grads[\"db2\"] = grads[\"db2\"].sum(0)\n",
    "\n",
    "    return grads\n",
    "\n",
    "TwoLayerNNRelu = copy.copy(TwoLayerNN)\n",
    "TwoLayerNNRelu.initialize_parameters = initialize_parameters\n",
    "\n",
    "TwoLayerNNRelu.feed_forward = forward_relu\n",
    "TwoLayerNNRelu.back_propagate = backward_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2qY3T98XvEIP"
   },
   "outputs": [],
   "source": [
    "### \n",
    "# Question (e)\n",
    "# Tune the hyperparameters with validation data,\n",
    "# and print the results by running the lines below.\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "h-jJRXqsBxzh"
   },
   "outputs": [],
   "source": [
    "# model instantiation\n",
    "model_relu = TwoLayerNNRelu(input_dim=784, num_hiddens=256, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EC8f80a0w53m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 00 - train loss/acc: 0.418 0.873, valid loss/acc: 0.421 0.869\n",
      "epoch 01 - train loss/acc: 0.326 0.906, valid loss/acc: 0.331 0.903\n",
      "epoch 02 - train loss/acc: 0.296 0.915, valid loss/acc: 0.299 0.912\n",
      "epoch 03 - train loss/acc: 0.284 0.918, valid loss/acc: 0.289 0.916\n",
      "epoch 04 - train loss/acc: 0.269 0.921, valid loss/acc: 0.277 0.919\n",
      "epoch 05 - train loss/acc: 0.252 0.926, valid loss/acc: 0.260 0.923\n",
      "epoch 06 - train loss/acc: 0.233 0.935, valid loss/acc: 0.242 0.930\n",
      "epoch 07 - train loss/acc: 0.223 0.936, valid loss/acc: 0.234 0.932\n",
      "epoch 08 - train loss/acc: 0.209 0.941, valid loss/acc: 0.220 0.938\n",
      "epoch 09 - train loss/acc: 0.195 0.944, valid loss/acc: 0.207 0.940\n",
      "epoch 10 - train loss/acc: 0.187 0.947, valid loss/acc: 0.200 0.943\n",
      "epoch 11 - train loss/acc: 0.177 0.950, valid loss/acc: 0.191 0.946\n",
      "epoch 12 - train loss/acc: 0.167 0.953, valid loss/acc: 0.181 0.949\n",
      "epoch 13 - train loss/acc: 0.160 0.955, valid loss/acc: 0.174 0.950\n",
      "epoch 14 - train loss/acc: 0.154 0.957, valid loss/acc: 0.169 0.952\n",
      "epoch 15 - train loss/acc: 0.145 0.960, valid loss/acc: 0.161 0.954\n",
      "epoch 16 - train loss/acc: 0.140 0.961, valid loss/acc: 0.157 0.955\n",
      "epoch 17 - train loss/acc: 0.138 0.961, valid loss/acc: 0.157 0.954\n",
      "epoch 18 - train loss/acc: 0.130 0.963, valid loss/acc: 0.148 0.958\n",
      "epoch 19 - train loss/acc: 0.123 0.966, valid loss/acc: 0.143 0.960\n",
      "epoch 20 - train loss/acc: 0.118 0.967, valid loss/acc: 0.139 0.961\n",
      "epoch 21 - train loss/acc: 0.117 0.967, valid loss/acc: 0.135 0.961\n",
      "epoch 22 - train loss/acc: 0.110 0.969, valid loss/acc: 0.132 0.962\n",
      "epoch 23 - train loss/acc: 0.106 0.970, valid loss/acc: 0.127 0.964\n",
      "epoch 24 - train loss/acc: 0.105 0.970, valid loss/acc: 0.127 0.964\n",
      "epoch 25 - train loss/acc: 0.100 0.972, valid loss/acc: 0.122 0.965\n",
      "epoch 26 - train loss/acc: 0.096 0.974, valid loss/acc: 0.118 0.967\n",
      "epoch 27 - train loss/acc: 0.093 0.974, valid loss/acc: 0.118 0.966\n",
      "epoch 28 - train loss/acc: 0.091 0.975, valid loss/acc: 0.115 0.967\n",
      "epoch 29 - train loss/acc: 0.088 0.976, valid loss/acc: 0.113 0.968\n",
      "epoch 30 - train loss/acc: 0.085 0.977, valid loss/acc: 0.110 0.970\n",
      "epoch 31 - train loss/acc: 0.084 0.976, valid loss/acc: 0.108 0.970\n",
      "epoch 32 - train loss/acc: 0.081 0.979, valid loss/acc: 0.107 0.969\n",
      "epoch 33 - train loss/acc: 0.077 0.979, valid loss/acc: 0.105 0.971\n",
      "epoch 34 - train loss/acc: 0.075 0.980, valid loss/acc: 0.102 0.972\n",
      "epoch 35 - train loss/acc: 0.074 0.980, valid loss/acc: 0.104 0.970\n",
      "epoch 36 - train loss/acc: 0.072 0.981, valid loss/acc: 0.101 0.971\n",
      "epoch 37 - train loss/acc: 0.069 0.982, valid loss/acc: 0.099 0.972\n",
      "epoch 38 - train loss/acc: 0.067 0.982, valid loss/acc: 0.097 0.973\n",
      "epoch 39 - train loss/acc: 0.065 0.982, valid loss/acc: 0.096 0.973\n",
      "epoch 40 - train loss/acc: 0.063 0.983, valid loss/acc: 0.094 0.973\n",
      "epoch 41 - train loss/acc: 0.063 0.983, valid loss/acc: 0.095 0.973\n",
      "epoch 42 - train loss/acc: 0.060 0.985, valid loss/acc: 0.091 0.975\n",
      "epoch 43 - train loss/acc: 0.059 0.985, valid loss/acc: 0.091 0.973\n",
      "epoch 44 - train loss/acc: 0.058 0.985, valid loss/acc: 0.090 0.974\n",
      "epoch 45 - train loss/acc: 0.055 0.986, valid loss/acc: 0.089 0.974\n",
      "epoch 46 - train loss/acc: 0.054 0.986, valid loss/acc: 0.088 0.975\n",
      "epoch 47 - train loss/acc: 0.053 0.987, valid loss/acc: 0.088 0.974\n",
      "epoch 48 - train loss/acc: 0.051 0.987, valid loss/acc: 0.086 0.975\n",
      "epoch 49 - train loss/acc: 0.050 0.987, valid loss/acc: 0.086 0.975\n",
      "epoch 50 - train loss/acc: 0.050 0.987, valid loss/acc: 0.085 0.975\n",
      "epoch 51 - train loss/acc: 0.048 0.988, valid loss/acc: 0.083 0.976\n",
      "epoch 52 - train loss/acc: 0.046 0.988, valid loss/acc: 0.083 0.976\n",
      "epoch 53 - train loss/acc: 0.046 0.989, valid loss/acc: 0.084 0.976\n",
      "epoch 54 - train loss/acc: 0.045 0.989, valid loss/acc: 0.082 0.976\n",
      "epoch 55 - train loss/acc: 0.043 0.990, valid loss/acc: 0.081 0.977\n",
      "epoch 56 - train loss/acc: 0.042 0.990, valid loss/acc: 0.080 0.977\n",
      "epoch 57 - train loss/acc: 0.041 0.990, valid loss/acc: 0.080 0.977\n",
      "epoch 58 - train loss/acc: 0.040 0.991, valid loss/acc: 0.080 0.977\n",
      "epoch 59 - train loss/acc: 0.040 0.991, valid loss/acc: 0.081 0.977\n",
      "epoch 60 - train loss/acc: 0.039 0.991, valid loss/acc: 0.078 0.977\n",
      "epoch 61 - train loss/acc: 0.038 0.991, valid loss/acc: 0.078 0.977\n",
      "epoch 62 - train loss/acc: 0.037 0.992, valid loss/acc: 0.077 0.978\n",
      "epoch 63 - train loss/acc: 0.037 0.992, valid loss/acc: 0.079 0.977\n",
      "epoch 64 - train loss/acc: 0.035 0.993, valid loss/acc: 0.077 0.978\n",
      "epoch 65 - train loss/acc: 0.035 0.992, valid loss/acc: 0.077 0.977\n",
      "epoch 66 - train loss/acc: 0.034 0.993, valid loss/acc: 0.075 0.977\n",
      "epoch 67 - train loss/acc: 0.033 0.993, valid loss/acc: 0.076 0.977\n",
      "epoch 68 - train loss/acc: 0.032 0.993, valid loss/acc: 0.075 0.977\n",
      "epoch 69 - train loss/acc: 0.032 0.993, valid loss/acc: 0.074 0.978\n",
      "epoch 70 - train loss/acc: 0.031 0.994, valid loss/acc: 0.075 0.978\n",
      "epoch 71 - train loss/acc: 0.030 0.994, valid loss/acc: 0.074 0.978\n",
      "epoch 72 - train loss/acc: 0.030 0.994, valid loss/acc: 0.075 0.977\n",
      "epoch 73 - train loss/acc: 0.029 0.994, valid loss/acc: 0.073 0.978\n",
      "epoch 74 - train loss/acc: 0.028 0.994, valid loss/acc: 0.074 0.978\n",
      "epoch 75 - train loss/acc: 0.028 0.995, valid loss/acc: 0.073 0.979\n",
      "epoch 76 - train loss/acc: 0.027 0.995, valid loss/acc: 0.072 0.979\n",
      "epoch 77 - train loss/acc: 0.026 0.995, valid loss/acc: 0.073 0.978\n",
      "epoch 78 - train loss/acc: 0.026 0.995, valid loss/acc: 0.072 0.978\n",
      "epoch 79 - train loss/acc: 0.026 0.995, valid loss/acc: 0.073 0.978\n",
      "epoch 80 - train loss/acc: 0.025 0.995, valid loss/acc: 0.071 0.979\n",
      "epoch 81 - train loss/acc: 0.025 0.996, valid loss/acc: 0.072 0.978\n",
      "epoch 82 - train loss/acc: 0.024 0.996, valid loss/acc: 0.072 0.979\n",
      "epoch 83 - train loss/acc: 0.024 0.996, valid loss/acc: 0.072 0.978\n",
      "epoch 84 - train loss/acc: 0.023 0.996, valid loss/acc: 0.070 0.979\n",
      "epoch 85 - train loss/acc: 0.023 0.996, valid loss/acc: 0.071 0.978\n",
      "epoch 86 - train loss/acc: 0.022 0.996, valid loss/acc: 0.071 0.979\n",
      "epoch 87 - train loss/acc: 0.022 0.996, valid loss/acc: 0.070 0.979\n",
      "epoch 88 - train loss/acc: 0.022 0.997, valid loss/acc: 0.070 0.978\n",
      "epoch 89 - train loss/acc: 0.021 0.997, valid loss/acc: 0.070 0.979\n",
      "epoch 90 - train loss/acc: 0.021 0.997, valid loss/acc: 0.072 0.978\n",
      "epoch 91 - train loss/acc: 0.020 0.997, valid loss/acc: 0.070 0.978\n",
      "epoch 92 - train loss/acc: 0.020 0.997, valid loss/acc: 0.070 0.979\n",
      "epoch 93 - train loss/acc: 0.020 0.997, valid loss/acc: 0.070 0.980\n",
      "epoch 94 - train loss/acc: 0.019 0.997, valid loss/acc: 0.070 0.979\n",
      "epoch 95 - train loss/acc: 0.019 0.997, valid loss/acc: 0.070 0.979\n",
      "epoch 96 - train loss/acc: 0.018 0.998, valid loss/acc: 0.069 0.979\n",
      "epoch 97 - train loss/acc: 0.018 0.998, valid loss/acc: 0.069 0.979\n",
      "epoch 98 - train loss/acc: 0.018 0.998, valid loss/acc: 0.070 0.979\n",
      "epoch 99 - train loss/acc: 0.017 0.998, valid loss/acc: 0.069 0.979\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "lr, n_epochs, batch_size =0.5, 100, 256\n",
    "history = model_relu.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4i__6TfpCqOc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test loss = 0.069, acc = 0.980\n"
     ]
    }
   ],
   "source": [
    "Y_hat, _ = model_relu.forward(X_test)\n",
    "test_loss = model_relu.compute_loss(Y_test, Y_hat)\n",
    "test_acc = model_relu.evaluate(Y_test, Y_hat)\n",
    "print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw3_2nn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
