{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDxtMC2g66gQ"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vj2CXov7JJqq"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DcKp4bZiJwut"
   },
   "outputs": [],
   "source": [
    "# %cd 'YOUR_DRIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UHKo6dP6eEO6"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.insert(0,str(Path().absolute().joinpath(\"data\")))\n",
    "\n",
    "from data import prepareData\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "########### import yours ###########\n",
    "\n",
    "####################################\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# TRAIN_RATIO: train dataset ratio, should be a float in (0, 0.8]\n",
    "# (0.8-TRAIN_RATIO) will be used for valid dataset\n",
    "TRAIN_RATIO = 0.6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kz2VM-9MIyKe"
   },
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qCY42PE0kRj"
   },
   "source": [
    "**Do NOT Modify** code blocks in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "OOnSsL1EeG85"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "UHo7XkIGIz2z"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, loss_fn, clip):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        if batch[0].shape[0]==BATCH_SIZE:\n",
    "            src = batch[0].to(device)\n",
    "            trg = batch[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(src, trg)\n",
    "            loss = loss_fn(output, trg)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "l2-jeXr-I1yP"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, loss_fn):\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "            if batch[0].shape[0]==BATCH_SIZE:\n",
    "                src = batch[0].to(device)\n",
    "                trg = batch[1].to(device)\n",
    "\n",
    "                output = model(src, trg)\n",
    "\n",
    "                loss = loss_fn(output, trg)\n",
    "                epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PgCsIyawXoN"
   },
   "source": [
    "## Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40ZOR80S0wet"
   },
   "source": [
    "**Do NOT Modify** code blocks in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "jwwJGiyETHsL"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "VALID_RATIO = 0.8-TRAIN_RATIO\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fePBsU2GKoaI",
    "outputId": "16074007-6d7b-441c-af2b-099a9bb11dc3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4345\n",
      "eng 2803\n",
      "data example\n",
      "['tu me fais de l ombre .', 'you re blocking my light .']\n"
     ]
    }
   ],
   "source": [
    "class TranslateDataset(Dataset):\n",
    "    def __init__(self, max_length=10, fra2eng=True):\n",
    "        self.input_lang, self.output_lang, self.pairs = prepareData('eng', 'fra', max_length=max_length, reverse=fra2eng)\n",
    "        self.max_length=max_length\n",
    "\n",
    "        self.input_lang.addWord('PAD')\n",
    "        self.output_lang.addWord('PAD')\n",
    "        self.input_lang_pad = self.input_lang.word2index['PAD']\n",
    "        self.output_lang_pad = self.output_lang.word2index['PAD']\n",
    "        \n",
    "        print(\"data example\")\n",
    "        print(random.choice(self.pairs))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        x, y = self._tensorsFromPair(pair)\n",
    "        return x, y\n",
    "\n",
    "    def _tensorFromSentence(self, lang, sentence):\n",
    "        indexes = [lang.word2index[word] for word in sentence.split(' ')]\n",
    "        indexes.append(EOS_token)\n",
    "        return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "    def _tensorsFromPair(self, pair):\n",
    "        input_tensor = self._tensorFromSentence(self.input_lang, pair[0])\n",
    "        target_tensor = self._tensorFromSentence(self.output_lang, pair[1])\n",
    "        return (input_tensor, target_tensor)\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        x_batch = []; y_batch = []\n",
    "        \n",
    "        for x, y in data:\n",
    "            if x.shape[0] < self.max_length-1:\n",
    "                x = torch.cat([x, self.input_lang_pad*torch.ones((self.max_length-1 - x.shape[0], 1), dtype=x.dtype)])\n",
    "            elif x.shape[0] > self.max_length-1:\n",
    "                x = x[:self.max_length-1]\n",
    "            if y.shape[0] < self.max_length-1:\n",
    "                y = torch.cat([y, self.output_lang_pad*torch.ones((self.max_length-1 - y.shape[0], 1), dtype=y.dtype)])\n",
    "            elif y.shape[0] > self.max_length-1:\n",
    "                y = y[:self.max_length-1]\n",
    "\n",
    "            x_batch.append(torch.cat([torch.tensor([SOS_token]), x.squeeze(1)]))\n",
    "            y_batch.append(torch.cat([torch.tensor([SOS_token]), y.squeeze(1)]))\n",
    "        \n",
    "        return torch.stack(x_batch), torch.stack(y_batch)\n",
    "\n",
    "dataset = TranslateDataset(max_length=MAX_LENGTH)\n",
    "\n",
    "train_size = int(len(dataset)*TRAIN_RATIO)\n",
    "valid_size = int(len(dataset)*VALID_RATIO)\n",
    "train_data, valid_data, test_data = random_split(dataset, [train_size, valid_size, len(dataset)-(train_size+valid_size)],)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=dataset.collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, collate_fn=dataset.collate_fn, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=dataset.collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TqutY58tG-h"
   },
   "source": [
    "# 1. Seq2Seq model with Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5xyf2mHuhmX"
   },
   "source": [
    "## Implement LSTM Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "1MM6lL95JcDa"
   },
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, emb_dim, hid_dim):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(in_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(input_size=emb_dim, hidden_size=hid_dim, num_layers=1, batch_first=True)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        '''\n",
    "        Q2 - (a)\n",
    "        Implement forward method of LSTM Encoder Module\n",
    "\n",
    "        INPUT\n",
    "        - input: input sentence, (B, max_len)\n",
    "        - hidden: initialized hidden state, (1, B, hid_dim)\n",
    "        - cell: initialized cell state, (1, B, hid_dim)\n",
    "\n",
    "        OUTPUT\n",
    "        What to be returned depends on your implementation of LSTMSeq2Seq. (Q2 - (b))\n",
    "        Feel free to return outputs you need.\n",
    "        some examples below\n",
    "        - hidden states of encoder\n",
    "\n",
    "        '''\n",
    "        ################### YOUR CODE ##|#################\n",
    "        hidden=hidden.to(DEVICE)\n",
    "        cell=cell.to(DEVICE)\n",
    "        \n",
    "        \n",
    "        embedded_input=self.embedding(input).to(DEVICE) #(B, max_len, emb_dim)\n",
    "        hiddens, (hidden, cell)=self.lstm(embedded_input, (hidden,cell)) #(B, 1, hid_dim)\n",
    "#         print(\"embedded_input (B, 1, hid_dim)\", hidden.shape)\n",
    "#         print(\"embedded_input (B, 1, hid_dim)\", cell.shape)\n",
    "        \n",
    "        \n",
    "        return hiddens, (hidden, cell)\n",
    "        #################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstme=LSTMEncoder(4346,512,256)\n",
    "# hid_dim=256\n",
    "# sample_x, sample_y = next(iter(train_dataloader))\n",
    "# sample_x = sample_x.squeeze(0)\n",
    "# sample_y = sample_y.squeeze(0)\n",
    "# embedded_x = nn.Embedding(4346, 512)(sample_x)\n",
    "# print(embedded_x.shape)\n",
    "# hidden_0 = torch.zeros(1,BATCH_SIZE, hid_dim)  # (1, Hout) for unbatched input\n",
    "# cell_0 =  torch.zeros(1,BATCH_SIZE,hid_dim)# (1, Hcell) for unbatched input\n",
    "\n",
    "# hiddens, (hidden, cell)=lstme(sample_x, hidden_0, cell_0)\n",
    "# print('LSTM Encoder outputs')\n",
    "# print(f'hiddens: {hiddens.shape}\\thidden: {hidden.shape}\\tcell: {cell.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "VPQnZdz5KKnN"
   },
   "outputs": [],
   "source": [
    "class AttnLSTMDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, hid_dim, out_dim, dropout, enc_hiddens=None):\n",
    "        super(AttnLSTMDecoder, self).__init__()\n",
    "        \n",
    "        self.t = 0 # (t)th token decoder\n",
    "        self.enc_hiddens = enc_hiddens # encoder output\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(out_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim + hid_dim, hid_dim, batch_first=True)\n",
    "        self.classifier = nn.Linear(hid_dim, out_dim)\n",
    "        self.cell_0 = torch.zeros(1, hid_dim)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        '''\n",
    "        Q2 - (a)\n",
    "        Implement forward method of LSTM Decoder Module with dot-product attention\n",
    "\n",
    "        INPUT\n",
    "        - input: input sentence (B, 1)\n",
    "        - hidden: previous hidden state (B, hid_dim)\n",
    "        - cell: previous cell state (1, B, hid_dim)\n",
    "\n",
    "        OUTPUT\n",
    "        What to be returned depends on your implementation of LSTMSeq2Seq. (Q2 - (b))\n",
    "        Feel free to return outputs you need.\n",
    "        Some examples below\n",
    "        - predicted token embedding (B, n_words of target language), (B, emb_dim), etc.\n",
    "        - current hidden state\n",
    "        - current cell state\n",
    "        '''\n",
    "\n",
    "        ################### YOUR CODE ###################\n",
    "        query = hidden # set query to calculate attention #(1,64,512)\n",
    "#         print(\"Query Dimension (1,64,512)\", query.shape)\n",
    "        # Attention\n",
    "        kv=self.enc_hiddens # (64,10,512)\n",
    "        \n",
    "#         print(\"Query Dimension (64,10,512)\", kv.shape)\n",
    "        \n",
    "        attn_score=torch.bmm(kv, torch.permute(query, (1,2,0)))\n",
    "        attn_coefficient =  F.softmax(attn_score, dim=0)#(64,10, 1)\n",
    "#         print(\"attn_coefficient Dimension (64,10, 1)\", attn_coefficient.shape)\n",
    "        \n",
    "        \n",
    "        weighted_kv = kv*attn_coefficient #(64,10,512)\n",
    "#         print(\"weighted_kv Dimension (64,10,512)\", weighted_kv.shape)\n",
    "        \n",
    "        weighted_sum = torch.sum(weighted_kv, dim=1) # (64,512)\n",
    "        \n",
    "        weighted_sum=torch.reshape(weighted_sum,(weighted_sum.shape[0],1,weighted_sum.shape[1]))\n",
    "        #(64,1,256)\n",
    "#         print(\"weighted_sum Dimension (64,1,512)\", weighted_sum.shape)\n",
    "        \n",
    "        embedded_input=self.embedding(input)\n",
    "        embedded_input=torch.reshape(embedded_input,(BATCH_SIZE, 1, hid_dim))\n",
    "#         print(\"embedded_input:\\t\",  embedded_input.shape)\n",
    "#         print(\"weighted_sum shape:\\t\",  weighted_sum.shape)\n",
    "        new_input=torch.cat((embedded_input, weighted_sum) ,dim=2) #(64,1,768)\n",
    "#         print(\"new_input shape:(64,1,768)\\t\",  new_input.shape)\n",
    "        \n",
    "        \n",
    "        hidden=torch.reshape(hidden, (1, BATCH_SIZE, hid_dim))\n",
    "        hiddens, (hidden, cell)=self.lstm(new_input, (hidden,cell)) #(hiddens: 64, 1, 256)\n",
    "        self.t += 1 # update time for each forward\n",
    "        prediction=self.classifier(hiddens.squeeze(0)) #(64,1,2804)\n",
    "        return prediction, hidden, cell\n",
    "        #################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_dim=2805\n",
    "# emb_dim=512\n",
    "# dec_embedder = nn.Embedding(out_dim, emb_dim)\n",
    "# embedded_y = dec_embedder(sample_y) # ground truth\n",
    "# lstm = nn.LSTM(emb_dim + hid_dim, hid_dim, batch_first=True)\n",
    "# hidden=torch.randn(64, 256)\n",
    "# cell=torch.randn(1,64,256)\n",
    "# input=torch.randint(1,100,(64,1))\n",
    "# x=torch.randn(1,64,256)\n",
    "# x=torch.permute(x, (1,2,0))\n",
    "# print(x.shape)\n",
    "# y=torch.randn(64,10,256)\n",
    "# sttn=torch.bmm(y,x)\n",
    "# attn_coefficient =  F.softmax(sttn, dim=0)\n",
    "# weighted_kv = y*attn_coefficient\n",
    "# print(weighted_kv.shape)\n",
    "# weighted_sum = torch.sum(weighted_kv, dim=1) # attention value . concat되는 친구\n",
    "# weighted_sum=torch.reshape(weighted_sum,(weighted_sum.shape[0],1,weighted_sum.shape[1]))\n",
    "# print(weighted_sum.shape)\n",
    "# embedded_input=dec_embedder(input)\n",
    "# print(embedded_input.shape)\n",
    "# new_input=torch.cat((embedded_input, weighted_sum) ,dim=2)\n",
    "# hidden=torch.reshape(hidden, (1,64,256))\n",
    "# print(\"1\",new_input.shape)\n",
    "# print(\"2\",hidden.shape)\n",
    "# print(\"3\",cell.shape)\n",
    "# hiddens, (hidden, cell)=lstm(new_input, (hidden,cell))\n",
    "# print(hiddens.shape)\n",
    "# print(hidden.shape)\n",
    "# print(cell.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "zJsJ3p2NLD6c"
   },
   "outputs": [],
   "source": [
    "# This code block has been referred to https://deep-learning-study.tistory.com/685\n",
    "\n",
    "class LSTMSeq2Seq(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, emb_dim, hid_dim, device, dropout):\n",
    "        super(LSTMSeq2Seq, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.encoder = LSTMEncoder(in_dim, emb_dim, hid_dim)\n",
    "        self.decoder = AttnLSTMDecoder(emb_dim, hid_dim, out_dim, dropout)\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        '''\n",
    "        Q2 - (b)\n",
    "        Implement forward method of LSTM Seq2Seq Module\n",
    "        (Decoder module should attend encoder's outputs using dot product.)\n",
    "        \n",
    "        INPUT\n",
    "        - src: source language batched data (B, max_len)\n",
    "        - trg: target language batched data (B, max_len)\n",
    "\n",
    "        OUTPUT\n",
    "        - output of one-hot prediction (B, out_dim, max_len)\n",
    "        '''\n",
    "        batch_size, mx_len = src.shape\n",
    "        ################### YOUR CODE ###################\n",
    "\n",
    "        # Encoder (start from zero-hidden & zero-cell states)\n",
    "        \n",
    "        \n",
    "        hidden_0 = torch.zeros(1,BATCH_SIZE, hid_dim)  # (1, Hout) for unbatched input\n",
    "        cell_0 =  torch.zeros(1,BATCH_SIZE,hid_dim)# (1, Hcell) for unbatched input\n",
    "        hiddens, (hidden, cell)=self.encoder(src, hidden_0, cell_0)\n",
    "        # Decoder\n",
    "        self.decoder.enc_hiddens = hiddens # set encoder's hidden states\n",
    "        outputs = torch.zeros(MAX_LENGTH, batch_size, out_dim).to(self.device) # to store each decoder's output\n",
    "        input=trg[:,0]\n",
    "        for t in range(1, MAX_LENGTH): # for each t'th token, get decoder outputs\n",
    "            output,hidden,cell=self.decoder(input, hidden, cell)\n",
    "            outputs[t]=torch.reshape(output,(batch_size, out_dim))\n",
    "            continue\n",
    "        outputs=torch.permute(outputs,(1,2,0))\n",
    "        self.decoder.t=0 # after for loop, reset decoder's time to evaluate properly\n",
    "        return outputs\n",
    "        ################### YOUR CODE ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block has been referred to https://deep-learning-study.tistory.com/685\n",
    "\n",
    "class LSTMSeq2Seqforeval(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, emb_dim, hid_dim, device, dropout):\n",
    "        super(LSTMSeq2Seqforeval, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.encoder = LSTMEncoder(in_dim, emb_dim, hid_dim)\n",
    "        self.decoder = AttnLSTMDecoder(emb_dim, hid_dim, out_dim, dropout)\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        '''\n",
    "        Q2 - (b)\n",
    "        Implement forward method of LSTM Seq2Seq Module\n",
    "        (Decoder module should attend encoder's outputs using dot product.)\n",
    "        \n",
    "        INPUT\n",
    "        - src: source language batched data (B, max_len)\n",
    "        - trg: target language batched data (B, max_len)\n",
    "\n",
    "        OUTPUT\n",
    "        - output of one-hot prediction (B, out_dim, max_len)\n",
    "        '''\n",
    "        batch_size, mx_len = src.shape\n",
    "        ################### YOUR CODE ###################\n",
    "\n",
    "        # Encoder (start from zero-hidden & zero-cell states)\n",
    "        \n",
    "        \n",
    "        hidden_0 = torch.zeros(1,BATCH_SIZE, hid_dim)  # (1, Hout) for unbatched input\n",
    "        cell_0 =  torch.zeros(1,BATCH_SIZE,hid_dim)# (1, Hcell) for unbatched input\n",
    "        hiddens, (hidden, cell)=self.encoder(src, hidden_0, cell_0)\n",
    "        # Decoder\n",
    "        self.decoder.enc_hiddens = hiddens # set encoder's hidden states\n",
    "        outputs = torch.zeros(MAX_LENGTH, batch_size, out_dim).to(self.device) # to store each decoder's output\n",
    "        input=trg[:,0]\n",
    "        print(input.shape)\n",
    "        for t in range(1, MAX_LENGTH): # for each t'th token, get decoder outputs\n",
    "            output,hidden,cell=self.decoder(input, hidden, cell)\n",
    "            outputs[t]=torch.reshape(output,(batch_size, out_dim))\n",
    "            print(output.shape)\n",
    "            input=output\n",
    "            continue\n",
    "        outputs=torch.permute(outputs,(1,2,0))\n",
    "        self.decoder.t=0 # after for loop, reset decoder's time to evaluate properly\n",
    "        return outputs\n",
    "        ################### YOUR CODE ###################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cu3WP3mYw3NV"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "B4YKhRe9d9qC",
    "outputId": "687c2cf4-1c0d-4fbe-bd3c-b42dcedd768f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ2 - (c)\\nTrain your Seq2Seq model and plot losses and perplexities.\\nUpon successful training, the test perplexity should be less than 5.\\nYou may use visualization libraries for plotting and modify training options such as hyperparameters and optimizer.\\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q2 - (c)\n",
    "Train your Seq2Seq model and plot losses and perplexities.\n",
    "Upon successful training, the test perplexity should be less than 5.\n",
    "You may use visualization libraries for plotting and modify training options such as hyperparameters and optimizer.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "1QQgN03XecUV"
   },
   "outputs": [],
   "source": [
    "in_dim = dataset.input_lang.n_words\n",
    "out_dim = dataset.output_lang.n_words\n",
    "hid_dim = 256\n",
    "emb_dim = 256\n",
    "dropout = 0.5\n",
    "learning_rate=1e-2\n",
    "N_EPOCHS = 100\n",
    "valid_every=1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMSeq2Seq(in_dim, out_dim, emb_dim, hid_dim, device, dropout).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = dataset.output_lang_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "JbSR6BZKf-6L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 4.732 | Train PPL: 113.571\n",
      "==========================\n",
      "\t Val. Loss: 4.421 |  Val. PPL:  83.207\n",
      "Epoch: 02\n",
      "\tTrain Loss: 4.425 | Train PPL:  83.523\n",
      "==========================\n",
      "\t Val. Loss: 4.255 |  Val. PPL:  70.491\n",
      "Epoch: 03\n",
      "\tTrain Loss: 4.279 | Train PPL:  72.151\n",
      "==========================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-509b3f980bb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mvalid_every\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==========================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_valid_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-2746c565c0d0>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, iterator, loss_fn)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-5f5dedc6f29f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_hiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhiddens\u001b[0m \u001b[0;31m# set encoder's hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# to store each decoder's output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# for each t'th token, get decoder outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train your model\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss = train(model, train_dataloader, optimizer, loss_fn, 1)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    \n",
    "    if epoch%valid_every==0:\n",
    "        print(\"==========================\")\n",
    "        valid_loss = evaluate(model, valid_dataloader, loss_fn)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            model.decoder.t=0\n",
    "            torch.save(model.state_dict(), 'lstm-attn-model.pt')\n",
    "\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "1QQgN03XecUV"
   },
   "outputs": [],
   "source": [
    "in_dim = dataset.input_lang.n_words\n",
    "out_dim = dataset.output_lang.n_words\n",
    "hid_dim = 256\n",
    "emb_dim = 256\n",
    "dropout = 0.7\n",
    "learning_rate=1e-3\n",
    "N_EPOCHS = 100\n",
    "valid_every=1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMSeq2Seq(in_dim, out_dim, emb_dim, hid_dim, device, dropout).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = dataset.output_lang_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "JbSR6BZKf-6L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 4.541 | Train PPL:  93.751\n",
      "==========================\n",
      "\t Val. Loss: 3.798 |  Val. PPL:  44.597\n",
      "Epoch: 02\n",
      "\tTrain Loss: 3.684 | Train PPL:  39.789\n",
      "==========================\n",
      "\t Val. Loss: 3.526 |  Val. PPL:  33.993\n",
      "Epoch: 03\n",
      "\tTrain Loss: 3.463 | Train PPL:  31.928\n",
      "==========================\n",
      "\t Val. Loss: 3.396 |  Val. PPL:  29.843\n",
      "Epoch: 04\n",
      "\tTrain Loss: 3.318 | Train PPL:  27.617\n",
      "==========================\n",
      "\t Val. Loss: 3.304 |  Val. PPL:  27.209\n",
      "Epoch: 05\n",
      "\tTrain Loss: 3.190 | Train PPL:  24.281\n",
      "==========================\n",
      "\t Val. Loss: 3.225 |  Val. PPL:  25.154\n",
      "Epoch: 06\n",
      "\tTrain Loss: 3.072 | Train PPL:  21.582\n",
      "==========================\n",
      "\t Val. Loss: 3.160 |  Val. PPL:  23.563\n",
      "Epoch: 07\n",
      "\tTrain Loss: 2.963 | Train PPL:  19.363\n",
      "==========================\n",
      "\t Val. Loss: 3.126 |  Val. PPL:  22.784\n",
      "Epoch: 08\n",
      "\tTrain Loss: 2.853 | Train PPL:  17.343\n",
      "==========================\n",
      "\t Val. Loss: 3.049 |  Val. PPL:  21.089\n",
      "Epoch: 09\n",
      "\tTrain Loss: 2.750 | Train PPL:  15.635\n",
      "==========================\n",
      "\t Val. Loss: 3.009 |  Val. PPL:  20.265\n",
      "Epoch: 10\n",
      "\tTrain Loss: 2.655 | Train PPL:  14.227\n",
      "==========================\n",
      "\t Val. Loss: 2.983 |  Val. PPL:  19.740\n",
      "Epoch: 11\n",
      "\tTrain Loss: 2.566 | Train PPL:  13.019\n",
      "==========================\n",
      "\t Val. Loss: 2.948 |  Val. PPL:  19.062\n",
      "Epoch: 12\n",
      "\tTrain Loss: 2.477 | Train PPL:  11.905\n",
      "==========================\n",
      "\t Val. Loss: 2.918 |  Val. PPL:  18.510\n",
      "Epoch: 13\n",
      "\tTrain Loss: 2.397 | Train PPL:  10.994\n",
      "==========================\n",
      "\t Val. Loss: 2.907 |  Val. PPL:  18.298\n",
      "Epoch: 14\n",
      "\tTrain Loss: 2.321 | Train PPL:  10.183\n",
      "==========================\n",
      "\t Val. Loss: 2.886 |  Val. PPL:  17.928\n",
      "Epoch: 15\n",
      "\tTrain Loss: 2.244 | Train PPL:   9.431\n",
      "==========================\n",
      "\t Val. Loss: 2.870 |  Val. PPL:  17.641\n",
      "Epoch: 16\n",
      "\tTrain Loss: 2.176 | Train PPL:   8.813\n",
      "==========================\n",
      "\t Val. Loss: 2.862 |  Val. PPL:  17.505\n",
      "Epoch: 17\n",
      "\tTrain Loss: 2.109 | Train PPL:   8.237\n",
      "==========================\n",
      "\t Val. Loss: 2.859 |  Val. PPL:  17.447\n",
      "Epoch: 18\n",
      "\tTrain Loss: 2.040 | Train PPL:   7.692\n",
      "==========================\n",
      "\t Val. Loss: 2.858 |  Val. PPL:  17.425\n",
      "Epoch: 19\n",
      "\tTrain Loss: 1.984 | Train PPL:   7.275\n",
      "==========================\n",
      "\t Val. Loss: 2.823 |  Val. PPL:  16.834\n",
      "Epoch: 20\n",
      "\tTrain Loss: 1.916 | Train PPL:   6.791\n",
      "==========================\n",
      "\t Val. Loss: 2.803 |  Val. PPL:  16.491\n",
      "Epoch: 21\n",
      "\tTrain Loss: 1.850 | Train PPL:   6.361\n",
      "==========================\n",
      "\t Val. Loss: 2.824 |  Val. PPL:  16.846\n",
      "Epoch: 22\n",
      "\tTrain Loss: 1.799 | Train PPL:   6.041\n",
      "==========================\n",
      "\t Val. Loss: 2.803 |  Val. PPL:  16.489\n",
      "Epoch: 23\n",
      "\tTrain Loss: 1.743 | Train PPL:   5.714\n",
      "==========================\n",
      "\t Val. Loss: 2.807 |  Val. PPL:  16.563\n",
      "Epoch: 24\n",
      "\tTrain Loss: 1.690 | Train PPL:   5.420\n",
      "==========================\n",
      "\t Val. Loss: 2.821 |  Val. PPL:  16.788\n",
      "Epoch: 25\n",
      "\tTrain Loss: 1.645 | Train PPL:   5.183\n",
      "==========================\n",
      "\t Val. Loss: 2.806 |  Val. PPL:  16.547\n",
      "Epoch: 26\n",
      "\tTrain Loss: 1.594 | Train PPL:   4.926\n",
      "==========================\n",
      "\t Val. Loss: 2.802 |  Val. PPL:  16.474\n",
      "Epoch: 27\n",
      "\tTrain Loss: 1.549 | Train PPL:   4.708\n",
      "==========================\n",
      "\t Val. Loss: 2.800 |  Val. PPL:  16.450\n",
      "Epoch: 28\n",
      "\tTrain Loss: 1.505 | Train PPL:   4.502\n",
      "==========================\n",
      "\t Val. Loss: 2.799 |  Val. PPL:  16.423\n",
      "Epoch: 29\n",
      "\tTrain Loss: 1.464 | Train PPL:   4.325\n",
      "==========================\n",
      "\t Val. Loss: 2.805 |  Val. PPL:  16.527\n",
      "Epoch: 30\n",
      "\tTrain Loss: 1.430 | Train PPL:   4.178\n",
      "==========================\n",
      "\t Val. Loss: 2.810 |  Val. PPL:  16.609\n",
      "Epoch: 31\n",
      "\tTrain Loss: 1.397 | Train PPL:   4.042\n",
      "==========================\n",
      "\t Val. Loss: 2.824 |  Val. PPL:  16.842\n",
      "Epoch: 32\n",
      "\tTrain Loss: 1.361 | Train PPL:   3.900\n",
      "==========================\n",
      "\t Val. Loss: 2.811 |  Val. PPL:  16.622\n",
      "Epoch: 33\n",
      "\tTrain Loss: 1.329 | Train PPL:   3.777\n",
      "==========================\n",
      "\t Val. Loss: 2.831 |  Val. PPL:  16.957\n",
      "Epoch: 34\n",
      "\tTrain Loss: 1.305 | Train PPL:   3.687\n",
      "==========================\n",
      "\t Val. Loss: 2.868 |  Val. PPL:  17.605\n",
      "Epoch: 35\n",
      "\tTrain Loss: 1.281 | Train PPL:   3.602\n",
      "==========================\n",
      "\t Val. Loss: 2.836 |  Val. PPL:  17.041\n",
      "Epoch: 36\n",
      "\tTrain Loss: 1.260 | Train PPL:   3.526\n",
      "==========================\n",
      "\t Val. Loss: 2.847 |  Val. PPL:  17.231\n",
      "Epoch: 37\n",
      "\tTrain Loss: 1.242 | Train PPL:   3.462\n",
      "==========================\n",
      "\t Val. Loss: 2.859 |  Val. PPL:  17.437\n",
      "Epoch: 38\n",
      "\tTrain Loss: 1.223 | Train PPL:   3.396\n",
      "==========================\n",
      "\t Val. Loss: 2.874 |  Val. PPL:  17.706\n",
      "Epoch: 39\n",
      "\tTrain Loss: 1.200 | Train PPL:   3.321\n",
      "==========================\n",
      "\t Val. Loss: 2.868 |  Val. PPL:  17.605\n",
      "Epoch: 40\n",
      "\tTrain Loss: 1.185 | Train PPL:   3.272\n",
      "==========================\n",
      "\t Val. Loss: 2.872 |  Val. PPL:  17.674\n",
      "Epoch: 41\n",
      "\tTrain Loss: 1.172 | Train PPL:   3.229\n",
      "==========================\n",
      "\t Val. Loss: 2.890 |  Val. PPL:  18.002\n",
      "Epoch: 42\n",
      "\tTrain Loss: 1.156 | Train PPL:   3.176\n",
      "==========================\n",
      "\t Val. Loss: 2.907 |  Val. PPL:  18.298\n",
      "Epoch: 43\n",
      "\tTrain Loss: 1.140 | Train PPL:   3.127\n",
      "==========================\n",
      "\t Val. Loss: 2.925 |  Val. PPL:  18.631\n",
      "Epoch: 44\n",
      "\tTrain Loss: 1.131 | Train PPL:   3.099\n",
      "==========================\n",
      "\t Val. Loss: 2.930 |  Val. PPL:  18.724\n",
      "Epoch: 45\n",
      "\tTrain Loss: 1.124 | Train PPL:   3.078\n",
      "==========================\n",
      "\t Val. Loss: 2.937 |  Val. PPL:  18.854\n",
      "Epoch: 46\n",
      "\tTrain Loss: 1.115 | Train PPL:   3.050\n",
      "==========================\n",
      "\t Val. Loss: 2.947 |  Val. PPL:  19.041\n",
      "Epoch: 47\n",
      "\tTrain Loss: 1.116 | Train PPL:   3.051\n",
      "==========================\n",
      "\t Val. Loss: 2.953 |  Val. PPL:  19.160\n",
      "Epoch: 48\n",
      "\tTrain Loss: 1.104 | Train PPL:   3.017\n",
      "==========================\n",
      "\t Val. Loss: 2.954 |  Val. PPL:  19.188\n",
      "Epoch: 49\n",
      "\tTrain Loss: 1.094 | Train PPL:   2.985\n",
      "==========================\n",
      "\t Val. Loss: 2.980 |  Val. PPL:  19.685\n",
      "Epoch: 50\n",
      "\tTrain Loss: 1.086 | Train PPL:   2.963\n",
      "==========================\n",
      "\t Val. Loss: 2.969 |  Val. PPL:  19.480\n",
      "Epoch: 51\n",
      "\tTrain Loss: 1.079 | Train PPL:   2.941\n",
      "==========================\n",
      "\t Val. Loss: 2.990 |  Val. PPL:  19.878\n",
      "Epoch: 52\n",
      "\tTrain Loss: 1.073 | Train PPL:   2.923\n",
      "==========================\n",
      "\t Val. Loss: 3.003 |  Val. PPL:  20.151\n",
      "Epoch: 53\n",
      "\tTrain Loss: 1.070 | Train PPL:   2.916\n",
      "==========================\n",
      "\t Val. Loss: 3.015 |  Val. PPL:  20.396\n",
      "Epoch: 54\n",
      "\tTrain Loss: 1.065 | Train PPL:   2.902\n",
      "==========================\n",
      "\t Val. Loss: 3.023 |  Val. PPL:  20.546\n",
      "Epoch: 55\n",
      "\tTrain Loss: 1.061 | Train PPL:   2.889\n",
      "==========================\n",
      "\t Val. Loss: 3.032 |  Val. PPL:  20.733\n",
      "Epoch: 56\n",
      "\tTrain Loss: 1.060 | Train PPL:   2.886\n",
      "==========================\n",
      "\t Val. Loss: 3.037 |  Val. PPL:  20.850\n",
      "Epoch: 57\n",
      "\tTrain Loss: 1.067 | Train PPL:   2.906\n",
      "==========================\n",
      "\t Val. Loss: 3.059 |  Val. PPL:  21.297\n",
      "Epoch: 58\n",
      "\tTrain Loss: 1.079 | Train PPL:   2.942\n",
      "==========================\n",
      "\t Val. Loss: 3.067 |  Val. PPL:  21.476\n",
      "Epoch: 59\n",
      "\tTrain Loss: 1.077 | Train PPL:   2.937\n",
      "==========================\n",
      "\t Val. Loss: 3.056 |  Val. PPL:  21.232\n",
      "Epoch: 60\n",
      "\tTrain Loss: 1.071 | Train PPL:   2.920\n",
      "==========================\n",
      "\t Val. Loss: 3.063 |  Val. PPL:  21.381\n",
      "Epoch: 61\n",
      "\tTrain Loss: 1.062 | Train PPL:   2.894\n",
      "==========================\n",
      "\t Val. Loss: 3.057 |  Val. PPL:  21.261\n",
      "Epoch: 62\n",
      "\tTrain Loss: 1.051 | Train PPL:   2.861\n",
      "==========================\n",
      "\t Val. Loss: 3.059 |  Val. PPL:  21.298\n",
      "Epoch: 63\n",
      "\tTrain Loss: 1.042 | Train PPL:   2.835\n",
      "==========================\n",
      "\t Val. Loss: 3.087 |  Val. PPL:  21.911\n",
      "Epoch: 64\n",
      "\tTrain Loss: 1.038 | Train PPL:   2.823\n",
      "==========================\n",
      "\t Val. Loss: 3.076 |  Val. PPL:  21.674\n",
      "Epoch: 65\n",
      "\tTrain Loss: 1.035 | Train PPL:   2.816\n",
      "==========================\n",
      "\t Val. Loss: 3.089 |  Val. PPL:  21.948\n",
      "Epoch: 66\n",
      "\tTrain Loss: 1.031 | Train PPL:   2.804\n",
      "==========================\n",
      "\t Val. Loss: 3.101 |  Val. PPL:  22.227\n",
      "Epoch: 67\n",
      "\tTrain Loss: 1.032 | Train PPL:   2.807\n",
      "==========================\n",
      "\t Val. Loss: 3.107 |  Val. PPL:  22.351\n",
      "Epoch: 68\n",
      "\tTrain Loss: 1.030 | Train PPL:   2.802\n",
      "==========================\n",
      "\t Val. Loss: 3.122 |  Val. PPL:  22.688\n",
      "Epoch: 69\n",
      "\tTrain Loss: 1.031 | Train PPL:   2.804\n",
      "==========================\n",
      "\t Val. Loss: 3.129 |  Val. PPL:  22.848\n",
      "Epoch: 70\n",
      "\tTrain Loss: 1.034 | Train PPL:   2.813\n",
      "==========================\n",
      "\t Val. Loss: 3.146 |  Val. PPL:  23.236\n",
      "Epoch: 71\n",
      "\tTrain Loss: 1.031 | Train PPL:   2.804\n",
      "==========================\n",
      "\t Val. Loss: 3.164 |  Val. PPL:  23.669\n",
      "Epoch: 72\n",
      "\tTrain Loss: 1.038 | Train PPL:   2.823\n",
      "==========================\n",
      "\t Val. Loss: 3.147 |  Val. PPL:  23.269\n",
      "Epoch: 73\n",
      "\tTrain Loss: 1.047 | Train PPL:   2.848\n",
      "==========================\n",
      "\t Val. Loss: 3.175 |  Val. PPL:  23.921\n",
      "Epoch: 74\n",
      "\tTrain Loss: 1.058 | Train PPL:   2.881\n",
      "==========================\n",
      "\t Val. Loss: 3.168 |  Val. PPL:  23.759\n",
      "Epoch: 75\n",
      "\tTrain Loss: 1.048 | Train PPL:   2.852\n",
      "==========================\n",
      "\t Val. Loss: 3.158 |  Val. PPL:  23.522\n",
      "Epoch: 76\n",
      "\tTrain Loss: 1.042 | Train PPL:   2.835\n",
      "==========================\n",
      "\t Val. Loss: 3.169 |  Val. PPL:  23.792\n",
      "Epoch: 77\n",
      "\tTrain Loss: 1.033 | Train PPL:   2.809\n",
      "==========================\n",
      "\t Val. Loss: 3.165 |  Val. PPL:  23.688\n",
      "Epoch: 78\n",
      "\tTrain Loss: 1.028 | Train PPL:   2.795\n",
      "==========================\n",
      "\t Val. Loss: 3.188 |  Val. PPL:  24.247\n",
      "Epoch: 79\n",
      "\tTrain Loss: 1.027 | Train PPL:   2.791\n",
      "==========================\n",
      "\t Val. Loss: 3.187 |  Val. PPL:  24.222\n",
      "Epoch: 80\n",
      "\tTrain Loss: 1.024 | Train PPL:   2.784\n",
      "==========================\n",
      "\t Val. Loss: 3.209 |  Val. PPL:  24.760\n",
      "Epoch: 81\n",
      "\tTrain Loss: 1.022 | Train PPL:   2.778\n",
      "==========================\n",
      "\t Val. Loss: 3.212 |  Val. PPL:  24.836\n",
      "Epoch: 82\n",
      "\tTrain Loss: 1.019 | Train PPL:   2.772\n",
      "==========================\n",
      "\t Val. Loss: 3.196 |  Val. PPL:  24.429\n",
      "Epoch: 83\n",
      "\tTrain Loss: 1.019 | Train PPL:   2.771\n",
      "==========================\n",
      "\t Val. Loss: 3.209 |  Val. PPL:  24.755\n",
      "Epoch: 84\n",
      "\tTrain Loss: 1.019 | Train PPL:   2.770\n",
      "==========================\n",
      "\t Val. Loss: 3.228 |  Val. PPL:  25.229\n",
      "Epoch: 85\n",
      "\tTrain Loss: 1.019 | Train PPL:   2.770\n",
      "==========================\n",
      "\t Val. Loss: 3.244 |  Val. PPL:  25.628\n",
      "Epoch: 86\n",
      "\tTrain Loss: 1.019 | Train PPL:   2.769\n",
      "==========================\n",
      "\t Val. Loss: 3.247 |  Val. PPL:  25.725\n",
      "Epoch: 87\n",
      "\tTrain Loss: 1.018 | Train PPL:   2.768\n",
      "==========================\n",
      "\t Val. Loss: 3.233 |  Val. PPL:  25.357\n",
      "Epoch: 88\n",
      "\tTrain Loss: 1.018 | Train PPL:   2.768\n",
      "==========================\n",
      "\t Val. Loss: 3.263 |  Val. PPL:  26.138\n",
      "Epoch: 89\n",
      "\tTrain Loss: 1.021 | Train PPL:   2.777\n",
      "==========================\n",
      "\t Val. Loss: 3.244 |  Val. PPL:  25.626\n",
      "Epoch: 90\n",
      "\tTrain Loss: 1.026 | Train PPL:   2.790\n",
      "==========================\n",
      "\t Val. Loss: 3.256 |  Val. PPL:  25.954\n",
      "Epoch: 91\n",
      "\tTrain Loss: 1.025 | Train PPL:   2.787\n",
      "==========================\n",
      "\t Val. Loss: 3.274 |  Val. PPL:  26.418\n",
      "Epoch: 92\n",
      "\tTrain Loss: 1.024 | Train PPL:   2.785\n",
      "==========================\n",
      "\t Val. Loss: 3.291 |  Val. PPL:  26.865\n",
      "Epoch: 93\n",
      "\tTrain Loss: 1.028 | Train PPL:   2.794\n",
      "==========================\n",
      "\t Val. Loss: 3.280 |  Val. PPL:  26.579\n",
      "Epoch: 94\n",
      "\tTrain Loss: 1.041 | Train PPL:   2.832\n",
      "==========================\n",
      "\t Val. Loss: 3.285 |  Val. PPL:  26.701\n",
      "Epoch: 95\n",
      "\tTrain Loss: 1.050 | Train PPL:   2.858\n",
      "==========================\n",
      "\t Val. Loss: 3.258 |  Val. PPL:  25.999\n",
      "Epoch: 96\n",
      "\tTrain Loss: 1.048 | Train PPL:   2.853\n",
      "==========================\n",
      "\t Val. Loss: 3.264 |  Val. PPL:  26.149\n",
      "Epoch: 97\n",
      "\tTrain Loss: 1.033 | Train PPL:   2.811\n",
      "==========================\n",
      "\t Val. Loss: 3.252 |  Val. PPL:  25.837\n",
      "Epoch: 98\n",
      "\tTrain Loss: 1.023 | Train PPL:   2.783\n",
      "==========================\n",
      "\t Val. Loss: 3.273 |  Val. PPL:  26.380\n",
      "Epoch: 99\n",
      "\tTrain Loss: 1.017 | Train PPL:   2.765\n",
      "==========================\n",
      "\t Val. Loss: 3.287 |  Val. PPL:  26.763\n",
      "Epoch: 100\n",
      "\tTrain Loss: 1.015 | Train PPL:   2.759\n",
      "==========================\n",
      "\t Val. Loss: 3.283 |  Val. PPL:  26.646\n"
     ]
    }
   ],
   "source": [
    "# Train your model\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss = train(model, train_dataloader, optimizer, loss_fn, 1)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    \n",
    "    if epoch%valid_every==0:\n",
    "        print(\"==========================\")\n",
    "        valid_loss = evaluate(model, valid_dataloader, loss_fn)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            model.decoder.t=0\n",
    "            torch.save(model.state_dict(), 'lstm-attn-model.pt')\n",
    "\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "hBXKAKZo2lSS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Test. Loss: 3.307 |  Test. PPL:  27.300\n"
     ]
    }
   ],
   "source": [
    "# Test your model\n",
    "\n",
    "loaded_model = LSTMSeq2Seq(in_dim, out_dim, emb_dim, hid_dim, device, dropout).to(device)\n",
    "loaded_model.load_state_dict(torch.load('lstm-attn-model.pt'))\n",
    "\n",
    "test_loss = evaluate(loaded_model, test_dataloader, loss_fn)\n",
    "print(f'\\t Test. Loss: {valid_loss:.3f} |  Test. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FD1SYbuKOKGN"
   },
   "source": [
    "## [Bonus] Implement GRU Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ei1fgjRveS4n"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Q2 - (d)\n",
    "Change the modules(encoder, decoder) in Seq2Seq model to GRU, and repeat (a)~(c).\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "2U9A9kh3OSWW"
   },
   "outputs": [],
   "source": [
    "class GRUEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, emb_dim, hid_dim):\n",
    "        super(GRUEncoder, self).__init__()\n",
    "        ################### YOUR CODE ###################\n",
    "        self.embedding = nn.Embedding(in_dim, emb_dim)\n",
    "        self.lstm = nn.GRU(input_size=emb_dim, hidden_size=hid_dim, num_layers=1, batch_first=True)\n",
    "\n",
    "        #################################################\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        ################### YOUR CODE ###################\n",
    "        hidden=hidden.to(DEVICE)\n",
    "        cell=cell.to(DEVICE)\n",
    "        \n",
    "        \n",
    "        embedded_input=self.embedding(input).to(DEVICE) #(B, max_len, emb_dim)\n",
    "        hiddens, (hidden, cell)=self.lstm(embedded_input, (hidden,cell)) #(B, 1, hid_dim)\n",
    "\n",
    "        return hiddens, (hidden, cell)\n",
    "        #################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "9-IRYjI6O0xb"
   },
   "outputs": [],
   "source": [
    "class AttnGRUDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, hid_dim, out_dim, dropout, enc_hiddens=None):\n",
    "        super(AttnGRUDecoder, self).__init__()\n",
    "        \n",
    "        ################### YOUR CODE ###################\n",
    "        self.t = 0 # (t)th token decoder\n",
    "        self.enc_hiddens = enc_hiddens # encoder output\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(out_dim, emb_dim)\n",
    "        self.lstm = nn.GRU(emb_dim + hid_dim, hid_dim, batch_first=True)\n",
    "        self.classifier = nn.Linear(hid_dim, out_dim)\n",
    "        self.cell_0 = torch.zeros(1, hid_dim)\n",
    "        #################################################\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        ################### YOUR CODE ###################\n",
    "        query = hidden # set query to calculate attention #(1,64,512)\n",
    "#         print(\"Query Dimension (1,64,512)\", query.shape)\n",
    "        # Attention\n",
    "        kv=self.enc_hiddens # (64,10,512)\n",
    "        \n",
    "#         print(\"Query Dimension (64,10,512)\", kv.shape)\n",
    "        \n",
    "        attn_score=torch.bmm(kv, torch.permute(query, (1,2,0)))\n",
    "        attn_coefficient =  F.softmax(attn_score, dim=0)#(64,10, 1)\n",
    "#         print(\"attn_coefficient Dimension (64,10, 1)\", attn_coefficient.shape)\n",
    "        \n",
    "        \n",
    "        weighted_kv = kv*attn_coefficient #(64,10,512)\n",
    "#         print(\"weighted_kv Dimension (64,10,512)\", weighted_kv.shape)\n",
    "        \n",
    "        weighted_sum = torch.sum(weighted_kv, dim=1) # (64,512)\n",
    "        \n",
    "        weighted_sum=torch.reshape(weighted_sum,(weighted_sum.shape[0],1,weighted_sum.shape[1]))\n",
    "        #(64,1,256)\n",
    "#         print(\"weighted_sum Dimension (64,1,512)\", weighted_sum.shape)\n",
    "        \n",
    "        embedded_input=self.embedding(input)\n",
    "        embedded_input=torch.reshape(embedded_input,(BATCH_SIZE, 1, hid_dim))\n",
    "#         print(\"embedded_input:\\t\",  embedded_input.shape)\n",
    "#         print(\"weighted_sum shape:\\t\",  weighted_sum.shape)\n",
    "        new_input=torch.cat((embedded_input, weighted_sum) ,dim=2) #(64,1,768)\n",
    "#         print(\"new_input shape:(64,1,768)\\t\",  new_input.shape)\n",
    "        \n",
    "        \n",
    "        hidden=torch.reshape(hidden, (1, BATCH_SIZE, hid_dim))\n",
    "        hiddens, (hidden, cell)=self.lstm(new_input, (hidden,cell)) #(hiddens: 64, 1, 256)\n",
    "        self.t += 1 # update time for each forward\n",
    "        prediction=self.classifier(hiddens.squeeze(0)) #(64,1,2804)\n",
    "        return prediction, hidden, cell\n",
    "        return \n",
    "        #################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "BOOj4_hhP83d"
   },
   "outputs": [],
   "source": [
    "class GRUSeq2Seq(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, emb_dim, hid_dim, device, dropout):\n",
    "        super(GRUSeq2Seq, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.encoder = GRUEncoder(in_dim, emb_dim, hid_dim)\n",
    "        self.decoder = AttnGRUDecoder(emb_dim, hid_dim, out_dim, dropout)\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        batch_size, mx_len = src.shape\n",
    " # Encoder (start from zero-hidden & zero-cell states)\n",
    "        \n",
    "        \n",
    "        self.hidden_0 = torch.zeros(BATCH_SIZE, hid_dim)  # (1, Hout) for unbatched input\n",
    "        self.cell_0 =  torch.zeros(1,BATCH_SIZE,hid_dim)# (1, Hcell) for unbatched input\n",
    "        hiddens, (hidden, cell)=self.encoder(src, hidden_0, cell_0)\n",
    "        # Decoder\n",
    "        self.decoder.enc_hiddens = hiddens # set encoder's hidden states\n",
    "        outputs = torch.zeros(MAX_LENGTH, batch_size, out_dim).to(self.device) # to store each decoder's output\n",
    "        input=trg[:,0]\n",
    "        for t in range(1, MAX_LENGTH): # for each t'th token, get decoder outputs\n",
    "            output,hidden,cell=self.decoder(input, hidden, cell)\n",
    "            outputs[t]=torch.reshape(output,(batch_size, out_dim))\n",
    "            continue\n",
    "        outputs=torch.permute(outputs,(1,2,0))\n",
    "        self.decoder.t=0 # after for loop, reset decoder's time to evaluate properly\n",
    "        return outputs\n",
    "        #################################################\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "usq_ohj4Qqf5"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "For batched 3-D input, hx and cx should also be 3-D but got (2-D, 3-D) tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-c960d622eccb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {epoch+1:02}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-12309d145880>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, loss_fn, clip)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-442cc15915d5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mhidden_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid_dim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1, Hout) for unbatched input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mcell_0\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhid_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# (1, Hcell) for unbatched input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mhiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_hiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhiddens\u001b[0m \u001b[0;31m# set encoder's hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-696c54f6f148>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, cell)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0membedded_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(B, max_len, emb_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mhiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(B, 1, hid_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;31m#         print(\"embedded_input (B, 1, hid_dim)\", hidden.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#         print(\"embedded_input (B, 1, hid_dim)\", cell.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    745\u001b[0m                         msg = (\"For batched 3-D input, hx and cx should \"\n\u001b[1;32m    746\u001b[0m                                f\"also be 3-D but got ({hx[0].dim()}-D, {hx[1].dim()}-D) tensors\")\n\u001b[0;32m--> 747\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: For batched 3-D input, hx and cx should also be 3-D but got (2-D, 3-D) tensors"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gru_model = GRUSeq2Seq(in_dim, out_dim, emb_dim, hid_dim, device, dropout).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = dataset.output_lang_pad)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss = train(model, train_dataloader, optimizer, loss_fn, 1)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    \n",
    "    if epoch%valid_every==0:\n",
    "        print(\"==========================\")\n",
    "        valid_loss = evaluate(model, valid_dataloader, loss_fn)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            model.decoder.t=0\n",
    "            torch.save(model.state_dict(), 'gru-attn-model.pt')\n",
    "\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "WrFYlXfiRCSn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Test. Loss: -332.651 |  Test. PPL:   0.000\n"
     ]
    }
   ],
   "source": [
    "loaded_model = GRUSeq2Seq(in_dim, out_dim, emb_dim, hid_dim, device, dropout).to(device)\n",
    "model.load_state_dict(torch.load('gru-attn-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_dataloader, loss_fn)\n",
    "print(f'\\t Test. Loss: {valid_loss:.3f} |  Test. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAZdJqTJXVcP"
   },
   "source": [
    "# 2. Seq2Seq model with Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t07UOwbpH7CZ"
   },
   "source": [
    "## Implement Transformer Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cvv3bN0qXX1I"
   },
   "outputs": [],
   "source": [
    "class TransEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, ff_dim, dropout, device, max_length = MAX_LENGTH):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        \n",
    "        encoder_layer = TransformerEncoderLayer(hid_dim, n_heads, ff_dim, dropout, batch_first=True)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, n_layers)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.tensor([hid_dim], device = device, dtype=torch.float32))\n",
    "        \n",
    "    def forward(self, src, pos_emb, src_mask):\n",
    "        '''\n",
    "        Q3 - (c)\n",
    "        Implement forward method of TransEncoder Module\n",
    "        (Use torch.nn.TransformerEncoder, torch.nn.TransformerEncoderLayer)\n",
    "        \n",
    "        INPUT\n",
    "        - src: source language batched data (B, max_len)\n",
    "        - pos_emb: positional embedding (max_len, hid_dim)\n",
    "        - src_mask: padding mask tensor for source sentences (B, max_len)\n",
    "\n",
    "        OUTPUT\n",
    "        What to be returned depends on your implementation of TransSeq2Seq.\n",
    "        Feel free to return outputs you need.\n",
    "        Some examples below,\n",
    "\n",
    "        - encoder output (B, max_len, hid_dim)\n",
    "        '''\n",
    "        batch_size, src_len = src.shape\n",
    "        #################### YOUR CODE ####################\n",
    "\n",
    "        \n",
    "        return None\n",
    "        ###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mKifSxjIWKk"
   },
   "outputs": [],
   "source": [
    "class TransDecoder(nn.Module):\n",
    "    def __init__(self, out_dim, hid_dim, n_layers, n_heads, ff_dim, dropout, device, max_length = MAX_LENGTH):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(out_dim, hid_dim)\n",
    "        \n",
    "        decoder_layer = TransformerDecoderLayer(hid_dim, n_heads, ff_dim, dropout, batch_first=True)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, n_layers)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, out_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.tensor([hid_dim], device = device, dtype=torch.float32))\n",
    "        \n",
    "    def forward(self, trg, pos_emb, enc_src, trg_mask, trg_sub_mask, src_mask):\n",
    "        '''\n",
    "        Q3 - (c)\n",
    "        Implement forward method of TransDecoder Module\n",
    "        (Use torch.nn.TransformerDecoder, torch.nn.TransformerDecoderLayer)\n",
    "        \n",
    "        INPUT\n",
    "        - trg: target language batched data (B, max_len)\n",
    "        - pos_emb: positional embedding (max_len, hid_dim)\n",
    "        - enc_src: encoder outputs (B, max_len, hid_dim)\n",
    "        - trg_mask: padding mask tensor for target sentences (B, max_len)\n",
    "        - trg_sub_mask: subsequent mask for target sentences (max_len, max_len)\n",
    "        - src_mask: padding mask tensor for source sentences (B, max_len)\n",
    "\n",
    "        OUTPUT\n",
    "        What to be returned depends on your implementation of TransSeq2Seq.\n",
    "        Feel free to return outputs you need.\n",
    "        Some examples below,\n",
    "\n",
    "        - decoder output (B, max_len, out_dim)\n",
    "        '''\n",
    "        batch_size, trg_len = trg.shape\n",
    "\n",
    "        #################### YOUR CODE ####################\n",
    "        \n",
    "        return None\n",
    "        ###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5VFP1-3IXmv"
   },
   "outputs": [],
   "source": [
    "class TransSeq2Seq(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hid_dim, ff_dim, n_layers, n_heads, dropout_p, device, max_length=MAX_LENGTH):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.hid_dim = hid_dim\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.encoder = TransEncoder(in_dim, hid_dim, n_layers[0], n_heads, ff_dim, dropout_p, device)\n",
    "        self.decoder = TransDecoder(out_dim, hid_dim, n_layers[1], n_heads, ff_dim, dropout_p, device)\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        '''\n",
    "        Q3 - (b)\n",
    "        Implement mask generating function\n",
    "\n",
    "        INPUT\n",
    "        - src: batched input sentences (B, max_len)\n",
    "\n",
    "        OUTPUT\n",
    "        - Boolean padding mask tensor (B, max_len)\n",
    "        '''\n",
    "        #################### YOUR CODE ####################\n",
    "        \n",
    "        return None\n",
    "        ###################################################\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        '''\n",
    "        Q3 - (b)\n",
    "        Implement mask generating function\n",
    "\n",
    "        INPUT\n",
    "        - trg: batched target sentences (B, max_len)\n",
    "\n",
    "        OUTPUT\n",
    "        - A tuple of a padding mask tensor and a subsequent mask tensor ((B, max_len), (max_len, max_len))\n",
    "        '''\n",
    "        #################### YOUR CODE ####################\n",
    "\n",
    "        return None\n",
    "        ###################################################\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        '''\n",
    "        Q3 - (c)\n",
    "        Implement forward method of TransSeq2Seq Module\n",
    "        \n",
    "        INPUT\n",
    "        - src: source language batched data (B, max_len)\n",
    "        - trg: target language batched data (B, max_len)\n",
    "\n",
    "        OUTPUT\n",
    "        - decoder output (B, out_dim, max_dim)\n",
    "        \n",
    "        '''\n",
    "        #################### YOUR CODE ####################\n",
    "        \n",
    "        return None\n",
    "        ###################################################\n",
    "    \n",
    "    \n",
    "    def get_pos_emb(self):\n",
    "        '''\n",
    "        Q3 - (a)\n",
    "        Implement absolute positional embedding\n",
    "\n",
    "        OUTPUT\n",
    "        - positional embedding tensor (max_len, hid_dim)\n",
    "        '''\n",
    "        #################### YOUR CODE ####################\n",
    "\n",
    "        return None\n",
    "        ###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqnvbugEIJ-h"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8XBEzSHaUdX"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Q3 - (d)\n",
    "Train your Seq2Seq model and plot losses and perplexities.\n",
    "Upon successful training, the test perplexity should be less than 2.\n",
    "You may use visualization libraries for plotting and modify training options such as hyperparameters and optimizer.\n",
    "\n",
    "Based on the results from lSTM(GRU)-based and transformer-based Seq2Seq models,\n",
    "briefly describe which approach is better and why.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdNx0Ol5ILbR"
   },
   "outputs": [],
   "source": [
    "in_dim = dataset.input_lang.n_words\n",
    "out_dim = dataset.output_lang.n_words\n",
    "hid_dim = 256\n",
    "ff_dim = 1024\n",
    "n_enc_layers = 4\n",
    "n_dec_layers = 4\n",
    "n_layers = [n_enc_layers, n_dec_layers]\n",
    "n_heads = 8\n",
    "dropout = 0.1\n",
    "\n",
    "learning_rate=1e-2\n",
    "N_EPOCHS = 100\n",
    "valid_every=5\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TransSeq2Seq(in_dim, out_dim, hid_dim, ff_dim, n_layers, n_heads, dropout, device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = dataset.output_lang_pad)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aES3_sBTIgnN"
   },
   "outputs": [],
   "source": [
    "# Train your model\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss = train(model, train_dataloader, optimizer, loss_fn, 1)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    \n",
    "    if epoch%valid_every==0:\n",
    "        print(\"==========================\")\n",
    "        valid_loss = evaluate(model, valid_dataloader, loss_fn)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            model.decoder.t=0\n",
    "            torch.save(model.state_dict(), 'transformer-model.pt')\n",
    "\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4M6sUc-aJKP3"
   },
   "outputs": [],
   "source": [
    "# Test your model\n",
    "loaded_model = TransSeq2Seq(in_dim, out_dim, hid_dim, ff_dim, n_layers, n_heads, dropout, device).to(device)\n",
    "loaded_model.load_state_dict(torch.load('transformer-model.pt'))\n",
    "\n",
    "test_loss = evaluate(loaded_model, test_dataloader, loss_fn)\n",
    "print(f'\\t Test. Loss: {valid_loss:.3f} |  Test. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCswrEKrLeiQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "EDxtMC2g66gQ",
    "Kz2VM-9MIyKe",
    "5PgCsIyawXoN",
    "8TqutY58tG-h",
    "A5xyf2mHuhmX",
    "cu3WP3mYw3NV",
    "FD1SYbuKOKGN",
    "qAZdJqTJXVcP",
    "t07UOwbpH7CZ",
    "KqnvbugEIJ-h"
   ],
   "name": "hw4_translation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
